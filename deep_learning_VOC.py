# -*- coding: utf-8 -*-
"""CV_Group26_finalassignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vqbFoD5BTJ-c7ftQJtzJYawUsAsh-jsj

# **Computer Vision Final Project - Group 26**

**Contributors:** 
- Robbe D'Hondt
- Karel De Smet
- Andrew Gordon
- Zdenko Heyvaert

**IMPORTANT NOTE:** the included model weights can be uploaded in the following directory. Later in the notebook, they are loaded from this directory so that training doesn't need to take place every runtime. The first cell below makes the directory.

**content/VOCdevkit/models** 

Please make sure that the upload of the model weights is finished before using the 'load_weights'-command later in this notebook. Not doing so will result in an error message interrupting the execution of the code.
"""

import os

#Create directory
base_path = "/content/VOCdevkit/models"
if not os.path.isdir(base_path):
  os.makedirs(base_path)

#CLASSIFICATION
#From scratch weights
!wget -nc https://www.dropbox.com/s/w6h50nq0zvzu5x7/classification_model_1_fromscratch.h5?dl=0 -O /content/VOCdevkit/models/classification_model_1_fromscratch.h5
#From scratch history
!wget -nc https://www.dropbox.com/s/iyu7hc0dpbj9u87/history_model_1_classification?dl=0 -O /content/VOCdevkit/models/history_model_1_classification
#Transfer learning weights
!wget -nc https://www.dropbox.com/s/qoxqt1bufwevtmb/classification_model_2_finetuning.h5?dl=0 -O /content/VOCdevkit/models/classification_model_2_finetuning.h5
#Transfer learning history
!wget -nc https://www.dropbox.com/s/bcce0e0kng2lwt5/history_model_2_classification?dl=0 -O /content/VOCdevkit/models/history_model_2_classification

#SEGMENTATION
#From scratch weights
!wget https://www.dropbox.com/s/3ehvclpcm7eazrn/segmentation_model_3_fromscratch%20%281%29.h5?dl=0 -O /content/VOCdevkit/models/segmentation_model_3_fromscratch.h5
#From scratch history
!wget https://www.dropbox.com/s/piz3dbdvdbkvsdo/history_model_3_segmentation%20%281%29?dl=0 -O /content/VOCdevkit/models//history_model_3_segmentation.json
#Transfer learning weights
!wget  https://www.dropbox.com/s/w3ni0y7du21imvu/segmentation_model_4_pretrained.h5?dl=0 -O /content/VOCdevkit/models/segmentation_model_4_pretrained.h5
#Transfer learning history
!wget https://www.dropbox.com/s/9hvlkd0029dxo3x/history_model_4_segmentation?dl=0 -O /content/VOCdevkit/models//history_model_4_segmentation.json

# ADVERSARIAL
# Weights trained on training dataset
!wget -q --show-progress https://www.dropbox.com/s/hniexkzooglep7q/advmodel.h5?dl=0 -O /content/VOCdevkit/models/advmodel.h5
# Weights trained on testing dataset
!wget -q --show-progress https://www.dropbox.com/s/cf858d1pyrhesly/advmodel_onTest.h5?dl=0 -O /content/VOCdevkit/models/advmodel_onTest.h5

"""# **1. Introduction**

In this notebook, we will apply different deep learning techniques to computer vision problems. The first part of the notebook focuses on classificatin and segmentation. For both purposes, a deep neural network is created from scratch and trained, as well as a pretrained network is finetuned. In the second part of the notebook, the weaknesses of the classification network is exploited. An encoder-decoder type of convolutional neural network (CNN) is built which is able to fool the classification network. 

This notebook is structured as follows:

1.   Introduction
2.   Build training, validation and test set
3.   Classification (multi-label)
4.   Semantic Segmentation (by class)
5.   Adversial Examples 
6.   Discussion

First of all, some necessary packages are imported:
"""

import cv2
import numpy as np
import xml.etree.ElementTree as ET
import random
import math
from google.colab.patches import cv2_imshow
import tensorflow as tf
print(tf.__version__)
import matplotlib.pyplot as plt

"""# **2. Build training, validation and test set**

In the cells below, the complete dataset is downloaded, from which the training, validation and test sets are constructed. The dataset consists of coloured images of various scenes with different object classes. There are 20 classes in total:

*   Person: person
*   Animal: bird, cat, cow, dog, horse, sheep
*   Vehicle: aeroplane, bicycle, boat, bus, car, motorbike, train
*   Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor
"""

!wget -nc http://host.robots.ox.ac.uk/pascal/VOC/voc2009/VOCtrainval_11-May-2009.tar
!tar -xf VOCtrainval_11-May-2009.tar --totals

voc_root_folder = "/content/VOCdevkit/VOC2009"

image_path = "/content/VOCdevkit/VOC2009/JPEGImages"
annotation_path = "/content/VOCdevkit/VOC2009/Annotations"
files_name = sorted(os.listdir(image_path))
dataset = []    #Complete dataset
labels = []     #Corresponding labels 

# Iterating over all the .jpg files and corresponding .xml annotation files
for filename_ in files_name:
  filename, extension= os.path.splitext(filename_)
  img_path =image_path+'/'+filename+'.jpg'
  xml_path =annotation_path+'/'+filename+'.xml'
  img = cv2.imread(img_path)
  if img is None:
	  pass
  dataset.append(img)
  tree = ET.parse(xml_path)
  root = tree.getroot()
  objects = root.iter('object')
  labels_ = []
  for obj in objects:    # One image can have multiple labels
    label = obj.find('name').text
    labels_.append(label)
  labels.append(labels_)

"""The complete data set needs to be split into training, validation and test sets. The validation set is used during the model fitting to evaluate the loss and other metrics, however the model is not fit with this data. The test set is completely unused during the training phase and is only used at the end to evaluate how well the model generalizes to new data.

In order to do this, an index array with length equal to the total amount of images is shuffled randomly. After this, the first 70% of this shuffled array will serve as the indices to take from the original dataset to construct the training set. The same procedure is then done for validation set (next 15%) and test set (last 15%). 

We have made sure that the different sets remain fixed by making use of a seed for the random generator. Setting the seed each runtime at the same value before using the random generator ensures that the generation of index array is always the same.  
"""

train_part = 0.70
val_part = 0.15
test_part = 0.15

index_array = np.arange(np.size(dataset))
random.seed(20)
random.shuffle(index_array)

startIndVal  = math.floor(np.size(index_array)*train_part)
startIndTest = math.floor(np.size(index_array)*(train_part+val_part))
train_indices = index_array[:startIndVal]
val_indices   = index_array[startIndVal : startIndTest]
test_indices  = index_array[startIndTest:]

x_train = [dataset[i] for i in train_indices]
x_val   = [dataset[i] for i in val_indices]
x_test  = [dataset[i] for i in test_indices]

labels_train = [labels[i] for i in train_indices]
labels_val   = [labels[i] for i in val_indices]
labels_test  = [labels[i] for i in test_indices]

"""Here you see one example from each set, together with its label(s) and image size. As you can see, an image can have multiple classes. In the classification task in the next section, we will therefore perform multi-label classification. """

cv2_imshow(x_train[0])
print(labels_train[0])
print(np.shape(x_train[0]))

cv2_imshow(x_val[4])
print(labels_val[4])
print(np.shape(x_val[4]))

cv2_imshow(x_test[2])
print(labels_test[2])
print(np.shape(x_test[2]))



"""# **3. Classification**

Convolutional neural networks are now capable of outperforming humans on some computer vision tasks, such as classifying images. However, they require a lot of data and time to train. 

A convolutional neural network is a deep learning algorithm that takes as input an image, assigns importance to various aspects/objects by use of the weights and biases and is eventually able to differentiate and classify the images. The role of the first layers is to reduce the dimensionality of the images without losing too much important or crucial information. The idea behind successive layerwise processing is that the deeper layers get to see more and more context and will be able to learn more complex features. Eventually, only qualitative features are left, which are critical for making good predictions. The role of the final layer, which is a fully connected layer, is to make a class prediction based on the features it received from previous layers.

In this section, we will first construct a neural network and train it *from scratch*. Later, we will use *transfer learning*, where we apply fine-tuning to an existing architecture.

## Preprocessing and data exploration
"""

from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Activation, Dropout, BatchNormalization
from keras.utils import to_categorical
from sklearn.preprocessing import MultiLabelBinarizer
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam

"""First of all, the training, validation and test data are preprocessed. Each image is resized to the same size, here $128 \times 128 \times 3$, and the pixel values are scaled so that all pixelvalues are between $0$ and $1$. """

height = 128
width = 128
depth = 3
inputShape = (height, width, depth)

trainX = []
valX = []

for i, image in enumerate(x_train):
    trainX.append(cv2.resize(image, dsize=(width, height), interpolation=cv2.INTER_AREA))
trainX = np.array(trainX, dtype="float") / 255.0
trainY = np.array(labels_train)

for i, image in enumerate(x_val):
    valX.append(cv2.resize(image, dsize=(width, height), interpolation=cv2.INTER_AREA))
valX = np.array(valX, dtype="float") / 255.0
valY = np.array(labels_val)

testX = []

for i, image in enumerate(x_test):
    testX.append(cv2.resize(image, dsize=(width, height), interpolation=cv2.INTER_AREA))
testX = np.array(testX, dtype="float") / 255.0
testY = np.array(labels_test)

print(np.shape(trainX[0]))
print(trainY[0])
print(np.shape(valX[4]))
print(valY[4])

"""Further preprocessing happens by using **data augmentation**, via the class `ImageDataGenerator`. Training a deep neural network on more various data can result in a more skillful model. The purpose of the data generator is to create variations of the input images, so that the networks always sees new variations of the data at every epoch. Variations of the images include for example flips, zooms, shifts, etc. of the same image. This way, the model will impove its ability to generalize what it has learned to new images.

The image data generator constructed below will later be used in the training command (`fit_generator`) of the model.
"""

aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,
	height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,
	horizontal_flip=True, fill_mode="nearest")

"""Next, it might be worthwhile to take a closer look at the class labels of the images. As we saw above, an image can have more than one label. Therefore, let us **visualize** the **class label distribution**. In order to do this, the textual class labels are first translated to binary values (1 if the image contains a certain class, 0 otherwise). Consequently, each image sample will have as target output a binary vector of length 20. A value in this vector is 1 if the corresponding class is present in the image, 0 otherwise. This shape of target output will later also be given to the convolutional neural network for training. """

mlb = MultiLabelBinarizer()
trainY = mlb.fit_transform(trainY)
valY = mlb.fit_transform(valY)
testY = mlb.fit_transform(testY)

print("Size training label matrix: ", np.shape(trainY))
print("Size validation label matrix: ", np.shape(valY))
print("\nClasses:")
for (i, label) in enumerate(mlb.classes_):
	print("{}. {}".format(i + 1, label))
print("\nExamples:")
print(trainY[0,:], " = ", labels_train[0])
print(valY[4,:], " = ", labels_val[4])

"""Now we can easily visualize the class label distribution."""

import seaborn as sns

class_distribution = np.sum(trainY,axis=0)
print(class_distribution)
fig=plt.figure(figsize=(12, 4), dpi= 80, facecolor='w', edgecolor='k')
plt.title("Class label distribution")
sns.set(style="white")
s = sns.barplot(x=mlb.classes_, y=class_distribution, palette="Blues_d")
s.set_xticklabels(mlb.classes_,rotation=45);

"""What we see in the histogram above is 2 types of **imbalances** in the dataset. First, there is imbalance accross different labels, e.g. 'person' label is much more present than any other label. Secondly, there is imbalance in positive and negative examples in many classes, e.g. 'cow' label is present in just a few tens or hundreds of the total images. 

The first type of imbalance can produce overfitting to highly represented classes, while the second type of imbalance can produce that a class will always be flagged as negative. The latter will thus result in a high accuracy for that particular class while just predicting negative for each image. 

Because of these imbalances, we can calculate **weight classes**. It is worth a try to use this during training, but it's not garanteed that it will improve the performance. 
"""

labels_count = dict()
for i, count in enumerate(class_distribution):
  labels_count[i] = count
total_count = sum(labels_count.values())
class_weights = {cls: total_count / count for cls, count in enumerate(labels_count.values())}
print(class_weights)

"""## **Model 1: from scratch**

Now it's time to construct the convolutional neural network itself. The CNN here is a more compact version of the VGGNet network (Simonyan and Zisserman, 2014). It is characterized by $3 \times 3$ convolutional layers stacked in increasing depth, maxpooling layers to reduce the size and, of course, fully connected layers at the end of the network. We will here discuss the used layers in this archtecture in more detail. 

For the first layers we have consecutively: 
1. The input to the first layer of the network is of course the input image of size $(128,128,3)$, as discussed before. This is then given to a first convolutional - ReLU - pooling sequence. 
2. So, the first layer that the input image encounters is a convolutional layer in which $32$ filters are used of size $3 \times 3$. The objective of a convolutional layer is to extract features. the first convolutional layers typically extract low-order features such as edges, color, gradient orientation, ... Later in the stack, more convolutional layers will follow, which will then extract higher-order features. 
3. We then use a ReLU activation function to introduce nonlinearities in the network. This is important, because introducing nonlinearities extends the kinds of functions that we can represent with our neural network. This particular activation function simply sets negative values to zero.
4. To prevent that data values become too big or too small, batch normalization is performed. The benefit of batch normalization is that the network will train faster and converge more quickly.
5. We then use a max-pooling layer that further reduces the image size, by taking the maximum value of each consequent $3 \times 3$ submatrix. This is important to reduce the computational power that is required to process all the data. Furthermore, by using max-pooling, more dominant features are extracted.
6. Finally, at the end of this block of layers we have a dropout layer. Dropout is the process of randomly disconnecting nodes from the current layer to the next layer. This process of random disconnects naturally helps the network to reduce overfitting, as no one single node in the layer will be responsible for predicting a certain class, object, edge, or corner.
"""

# Initialize the model
model_1 = Sequential()

# First block of layers:
model_1.add(Conv2D(32, (3, 3), padding="same", input_shape=inputShape))
model_1.add(Activation("relu"))
model_1.add(BatchNormalization(axis=-1))
model_1.add(MaxPooling2D(pool_size=(3, 3)))
model_1.add(Dropout(0.25))

"""Basically, this is more or less repeated several times in order to be able to learn more complex features. The filters, kernels, and pool sizes in this code block work together to progressively reduce the spatial size but increase depth. The deeper in the network, the more complex extracted features become. """

model_1.add(Conv2D(64, (3, 3), padding="same"))
model_1.add(Activation("relu"))
model_1.add(BatchNormalization(axis=-1))

model_1.add(Conv2D(64, (3, 3), padding="same"))
model_1.add(Activation("relu"))
model_1.add(BatchNormalization(axis=-1))
model_1.add(MaxPooling2D(pool_size=(2, 2)))
model_1.add(Dropout(0.25))

model_1.add(Conv2D(128, (3, 3), padding="same"))
model_1.add(Activation("relu"))
model_1.add(BatchNormalization(axis=-1))

model_1.add(Conv2D(128, (3, 3), padding="same"))
model_1.add(Activation("relu"))
model_1.add(BatchNormalization(axis=-1))
model_1.add(MaxPooling2D(pool_size=(2, 2)))
model_1.add(Dropout(0.25))

"""Finally, the multi-dimensional data is converted to a vector with a `Flatten` layer, before the fully connected `Dense` layers are placed. The last layer eventually has 20 neurons, equal to the number of classes. 

For the final activation, the 'sigmoid' activation function is used because we are doing multi-label classification here. Softmax isn’t a good activation as it has the tendency to classify an image in just 1 of the categories. Our intention is rather to predicted all classes that are present in a particular image. 
"""

# Fully connected layers
model_1.add(Flatten())
model_1.add(Dense(512))
model_1.add(Activation("relu"))
model_1.add(BatchNormalization())
model_1.add(Dropout(0.5))
model_1.add(Dense(20))
model_1.add(Activation("sigmoid"))

"""The complete model is summarized here:"""

model_1.summary()

"""In addition to accuracy, the precision, recall and f1 score will be calculated during training for training and validation set. The f1-score is actually a measure that combines both precision and recall.

In contrast to a binary or multi-class classifier, a misclassification in multi-label classification is not necessarily hard wrong or right. A prediction containing a subset of the actual classes should be considered better than a prediction that contains none of them, e.g. predicting 2 of 3 labels correct is better than no correct labels at all. That's why the standard accuracy isn't really a good measure to evaluate the quality of prediction. The accuracy is still shown during training though, because it makes sense in a way as training performance measure and can be used to monitor overfitting. However, for the eventual evaluation on the test set after learning, we will not use accuracy. 
"""

from keras import backend as K

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

"""The Adam optimizer is used during training. We also set the number of epochs here for training. Too many epochs can lead to overfitting of the model, whereas too few may result in an underfit model."""

epochs = 50
o = Adam(learning_rate=0.001, decay=0.001 / epochs)

"""Eventually, the model needs to be compiled. As loss function, the `binary_crossentropy` loss function is used. In this case, we are actually doing 20 binary classifications, one for each label. Remember that an image can have one or more labels, so each output should be treated independently. The result with this loss function will be that for each label we will have a probability for a label being present in the image, indepently of the probabilities of other labels.

"""

model_1.compile(loss="binary_crossentropy", optimizer=o, metrics=["accuracy", f1_m, precision_m, recall_m])

"""IMPORTANT NOTE: If the model is already trained before and the weights and models were saved, they can be loaded to the model with following line of code. Take care that the code cell after this one is then commented, as this would again train the model."""

model_1.load_weights("/content/VOCdevkit/models/classification_model_1_fromscratch.h5")

"""The model will be trained in the code cell below. Please (un)comment this piece of code if necessary (see note above).

Eventually, the class weights didn't really improve the performance of the model. 
"""

from keras.callbacks import EarlyStopping

batch_size = 32

# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)    


# history = model_1.fit_generator(
#     aug.flow(trainX, trainY, batch_size=batch_size), 
#     epochs=epochs, 
#     validation_data=(valX, valY), 
#     # class_weight=class_weights, 
#     # callbacks=[es],
#     steps_per_epoch=len(trainX) // batch_size, 
#     verbose=1)

"""History is saved here:"""

import pickle

# with open('/content/VOCdevkit/models/history_model_1_classification', 'wb') as file_pi:
#   pickle.dump(history.history, file_pi)

"""One can save the learned complete model by uncommenting the following code: """

#model_1.save("/content/VOCdevkit/models/model_1_fromscratch_50epochs.h5")

"""One can save the learned the learned model weights with following code: """

# model_1.save_weights("/content/VOCdevkit/models/classification_model_1_fromscratch.h5")

"""When training is finished, we can plot the loss and accuracy on training and validation set during training. 

It's important that our model does not overfit or underfit and is be able to generalize well from our training data to unseen data (e.g. the test set). Therefore, the accuracy and loss on training and validation set are plotted below. 

Our model would underfit if the accuracy on the validation set would be than the accuracy of the training set. Often, underfitting in a deep neural network is caused by dropout layers. Too much dropout can encourage underfitting. We see however in the accuracy curve below that our model is not underfit.  

On the other hand, overfitting occurs when the model is too well on the training set, so that it becomes difficult for the model to generalize to new examples that were not in the training set. In this case, the trianing accuracy would be higher than the validation accuracy. There are some things that help to reduce overfitting. Obviously, adding more data would help, but this is often not possible. Another thing is is to use data augmentation, which we did. The use of a right amount of dropout also helps to prevent overfitting, which we also did. Lastly, early stopping is a method to prevent overfitting. Early stopping rules provide guidance as to how many iterations can be run before the learner begins to overfit.

We can see in the plots that the accuracy on the training set keeps increasing while accuracy on the validation set doesn't anymore. This is a sign the model was about to start overfitting.

"""

pickle_in = open("/content/VOCdevkit/models/history_model_1_classification","rb")
history_1 = pickle.load(pickle_in)

def plot_learningcurve(history, epoch):
  #plot accuracy
  epoch_range = range(1,epoch+1)
  plt.plot(epoch_range, history['accuracy'])
  plt.plot(epoch_range, history['val_accuracy'])
  plt.title('Model accuracy')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Val'], loc = 'upper left')
  plt.show()

  #plot loss:
  plt.plot(epoch_range, history['loss'])
  plt.plot(epoch_range, history['val_loss'])
  plt.title('Model loss')
  plt.ylabel('Loss')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Val'], loc = 'upper left')
  plt.show()

plot_learningcurve(history_1, epochs)

"""**Test set evaluation:**

Let's now evaluate how this model performs on unseen data.
"""

import copy

images_test = copy.deepcopy(x_test)

"""We use our own trained model to predict the labels of the images in the test set. Remember that our model provides a probability for each class being present in the image. A threshold is then used to determine from which level of probability we classify the image as containing a particular class."""

proba = model_1.predict(testX)
idxs = np.argsort(proba)[:,::-1][:,:5]

"""The test set was copied in a different array, so that we can put the labels together with its probability on the images, which makes it easy to see which are the most predicted labels. """

for (k, image) in enumerate(images_test):
  row = idxs[k,:]
  for (i, j) in enumerate(row):
    # build the label and draw the label on the image
    label = "{}: {:.2f}%".format(mlb.classes_[j], proba[k,j] * 100)
    cv2.putText(image, label, (10, (i * 30) + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

"""Before we start with evaluating the performance on the complete test set, we can take a look at some examples of predictions. Below you can see some examples of very good predictions. 

For clarity, the green labels written on the images are the predictions, the true labels are always printed below the images. 
"""

cv2_imshow(images_test[2])
print(labels_test[2])
cv2_imshow(images_test[0])
print(labels_test[0])
cv2_imshow(images_test[10])
print(labels_test[10])
cv2_imshow(images_test[12])
print(labels_test[12])
cv2_imshow(images_test[15])
print(labels_test[15])

test_loss, test_accuracy, test_f1, test_precision, test_recall = model_1.evaluate(testX,testY)
print("test set loss: ", test_loss)
print("test set accuracy", test_accuracy)
print("test set f1-sore", test_f1)
print("test set precision", test_precision)
print("test set recall", test_recall)

"""Now the precision, recall and f1-score is calculated for all classes separately. We see that for some classes, like 'person', 'train' and 'aeroplane', the performance is not bad. For other classes however, we see a very bad performance. 'bottle' for example has both precision and recall of almost zero, which basically means that almost no prediction was correct.

As pointed out before, accuracy wouldn't the best evaluation metric here. With imbalanced classes, it’s easy to get a high accuracy without actually making useful predictions. As you can see above, overall test set accuracy is **93.8%**, but taking a closer look per class and using other metrics, this is not really representative for the multi-label classification. This is mainly because accuracy takes for one example all labels that are true zero and predicted zero, as correctly predicted. But since for all images most of the labels are zero, the labels that are predicted zero as well are then included in the accuracy measure. For multi-label classification we are however more interested in how many of the true labels are predicted correctly, instead of also included how many of the not present labels are predicted correctly. 

As you can see, the micro-average f1-score for this classifier is around **0.49**. We see a decent precision score, which basically means that among the positive predictions there were effectively quite some true positives and less false positives. The recall however is that good, meaning that we still miss many relevant elements in our predictions, so many false negatives.  
"""

from sklearn import metrics

predictions_1 = np.round(model_1.predict(testX),0)
classes = mlb.classes_
test_metrics = metrics.classification_report(testY,predictions_1, target_names = classes)
print(test_metrics)

"""We can visualize this with different confusion matrices, one for each class. This learns us some interesting things. For example, we see that for the 'dog' class, the model is always predicting negative.
Also here, the class imbalances are clearly visible. For almost every class, there are much more negative examples then postive ones.
"""

from sklearn.metrics import multilabel_confusion_matrix
import sys
import itertools

def plot_confusion_matrix(cm, classes, title, ax):

    ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        ax.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > cm.max() / 2. else "black")

    tick_marks = np.arange(len(classes))
    ax.set_xticks(tick_marks), ax.xaxis.set_ticklabels(classes)
    ax.set_yticks(tick_marks), ax.yaxis.set_ticklabels(classes)
    ax.set_xlabel('Predicted')
    ax.set_ylabel('Truth')
    ax.set_title(title)
    ax.grid(False)
    
def plot_multiclass_confusion_matrix(y_true, y_pred, classes, save_plot=False):
    fig, axes = plt.subplots(int(np.ceil(len(classes) / 2)), 2, figsize=(8, 32))
    axes = axes.flatten()
    for i, conf_matrix in enumerate(multilabel_confusion_matrix(y_true, y_pred)):
        tn, fp, fn, tp = conf_matrix.ravel()
        f1 = 2 * tp / (2 * tp + fp + fn + sys.float_info.epsilon)
        recall = tp / (tp + fn + sys.float_info.epsilon)
        precision = tp / (tp + fp + sys.float_info.epsilon)
        plot_confusion_matrix(
            np.array([[tp, fn], [fp, tn]]),
            classes=['+', '-'],
            title=f'Label: {classes[i]}\nf1={f1:.5f}\nrecall={recall:.5f}\nprecision={precision:.5f}',
            ax=axes[i]
        )
        plt.tight_layout()
    if save_plot:
        plt.savefig('confusion_matrices.png', dpi=50)

plot_multiclass_confusion_matrix(testY, predictions_1, mlb.classes_, save_plot=False)

"""## **Model 2: transfer learning**

Next, we will use a pre-trained network and finetune it with our current training set. In order to do this, we use a convolutional neural network that has already been trained, on the ImageNet dataset. This set has over 14 million images and 1000 classes. Since our dataset is not drastically different in context from the original ImageNet dataset, the pre-trained model will already have learned features that are relevant to our own classification problem. This network, pre-trained on a large and diverse dataset like ImageNet, captures universal features like curves and edges in its early layers, that are relevant and useful for our data set.

As the first layers in this network are able to extract features that are also relevant to our dataset, we freeze the weights in these first layers. We want to keep these weights intact, as they learn the more general features, that are not dataset-specific. The higher convolutional layers will be made trainable, so that the network focuses on learning dataset-specific features.  Moreover, as we only have 20 classes, the last fully connected layers have to be adapted so that only 20 classes need to be classified instead of 1000.
"""

height = 224
width = 224
depth = 3
inputShape = (height, width, depth)

trainX = []
valX = []
testX = []

for i, image in enumerate(x_train):
    trainX.append(cv2.resize(image, dsize=(width, height), interpolation=cv2.INTER_AREA))
trainX = np.array(trainX, dtype="float") / 255.0

for i, image in enumerate(x_val):
    valX.append(cv2.resize(image, dsize=(width, height), interpolation=cv2.INTER_AREA))
valX = np.array(valX, dtype="float") / 255.0

for i, image in enumerate(x_test):
    testX.append(cv2.resize(image, dsize=(width, height), interpolation=cv2.INTER_AREA))
testX = np.array(testX, dtype="float") / 255.0

"""In the code below, the pre-trained model is loaded. The used model is VGG16, who won the ILSVRC (ImageNet) competition in 2014. When loading this model, we exclude the fully connected layers at the top, as we will have to adapt these to classify 20 classess only. """

from keras.applications import VGG16
from keras.layers import Input
# from keras.applications.inception_v3 import InceptionV3

baseModel = VGG16(weights="imagenet", include_top=False, input_tensor=Input(shape=(height, width, depth)))

"""Now, we create the fully connected layers that will come at the top of the network. """

headModel = baseModel.output
headModel = Flatten(name="flatten")(headModel)
headModel = Dense(512, activation="relu")(headModel)
headModel = Dropout(0.5)(headModel)
headModel = Dense(20, activation="sigmoid")(headModel)

"""Everything is now put together in a single model (VGG16 base model and selfmade fully connected layers)."""

from keras.models import Model

model_2 = Model(inputs=baseModel.input, outputs=headModel)
for layer in baseModel.layers:
	layer.trainable = False

"""It's important to freeze the weights up to a certain layer, as they are already pre-trained to extract features that are also relevant to our dataset. """

for layer in baseModel.layers[15:]:
	layer.trainable = True

for layer in model_2.layers:
	print("{}: {}".format(layer, layer.trainable))

"""For the optimizer, we use a smaller learning rate to train the network. Since we expect the pre-trained weights to be quite good already as compared to randomly initialized weights, we do not want to distort them too quickly and too much."""

model_2.compile(loss="binary_crossentropy", optimizer=Adam(lr=1e-4), metrics=["accuracy"])

"""Eventually, the model is trained. """

epochs = 10
batch_size = 32

# history_2 = model_2.fit_generator(
# 	aug.flow(trainX, trainY, batch_size=batch_size),
# 	epochs=epochs,
# 	validation_data=(valX, valY),
# 	# class_weight=class_weights,
# 	steps_per_epoch=len(trainX) // batch_size
# 	)

"""Here the weights are saved:"""

# with open('/content/VOCdevkit/models/history_model_2_classification', 'wb') as file_pi:
#   pickle.dump(history_2.history, file_pi)

"""Saved weights can be loaded to the model here:"""

model_2.load_weights("/content/VOCdevkit/models/classification_model_2_finetuning.h5")

"""The model weights can be save here: """

# model_2.save_weights("/content/VOCdevkit/models/classification_model_2_fine.h5")

"""The learning curves are now plotted below. We see that even 10 epochs is already enough, as the training accuracy start to being higher than the validation accuracy, a sign that the model start to overfit. """

pickle_in = open("/content/VOCdevkit/models/history_model_2_classification","rb")
history_2 = pickle.load(pickle_in)

plot_learningcurve(history_2, epochs)

"""The class probabilities are again sorted from high to low. """

proba_2 = model_2.predict(testX)
idxs_2 = np.argsort(proba_2)[:,::-1][:,:5]

images_test_2 = copy.deepcopy(x_test)
for (k, image) in enumerate(images_test_2):
  row = idxs_2[k,:]
  for (i, j) in enumerate(row):
    # build the label and draw the label on the image
    label = "{}: {:.2f}%".format(mlb.classes_[j], proba_2[k,j] * 100)
    cv2.putText(image, label, (10, (i * 30) + 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

"""Again some results on the test set are plotted. We see that for the same 4 images the predictions are correct with even stronger probabilities. This could already give some indication that this model performs better than the model from scratch. """

cv2_imshow(images_test_2[2])
print(labels_test[2])
cv2_imshow(images_test_2[0])
print(labels_test[0])
cv2_imshow(images_test_2[10])
print(labels_test[10])
cv2_imshow(images_test_2[12])
print(labels_test[12])
cv2_imshow(images_test_2[15])
print(labels_test[15])

"""We see a test set accuracy of almost $\mathbf{96}\%$, which is higher than the performance of the previous model. But as already mentioned a couple of times, accuracy isn't the best metric for this unbalances multi-label classification problem."""

test_loss, test_accuracy = model_2.evaluate(testX,testY)
print("test set loss: ", test_loss)
print("test set accuracy", test_accuracy)

"""We see a F1-score of around $\mathbf{0.71}\%$ now, which is significantly better than the F1-score of the model from scratch. This shows that using pre-trained weights on a larger dataset effectively helped improving the performance. """

from sklearn import metrics

predictions_2 = np.round(model_2.predict(testX),0)
classes = mlb.classes_
test_metrics = metrics.classification_report(testY, predictions_2, target_names = classes)
print(test_metrics)

"""Summarized, in case of the finetuned model, we notice a test set accuracy of almost $\mathbf{96}\%$ and a micro-average F1-score of $\mathbf{0.71}$. This is a significant improvement compared to the model constructed from scratch. This also makes clear that accuracy is not the best evaluation metric in this case, as the difference is negligible, while the difference in F1-score is quite large.

Finally, also the confusion matrix show that there is some improvement. We see more colored squares on the diagonals now, which is better.
"""

plot_multiclass_confusion_matrix(testY, predictions_2, mlb.classes_, save_plot=False)

"""# **4. Semantic Segmentation**

Segmentation is the next natural stage of Computer Vision, leading on from Classification and Object Detection.

This progression can be seen as following:

Classification labels an image as containing a type of object - for example, "This image contains a car."

Object Detection locates an object in the image, detecting distinct objects and placing a bounding box around them to locate them - for example "This image contains a car here, and also a car here."

Segmentation, leading on from this, seeks to classify every pixel in an image - for example, every pixel which makes up each car inside the image will be classified as part of a car.

The process of achieving this includes both classification and object detection techniques. It allows us to locate objects with an image, but also know the precise outline of that image.

Furthermore, there are two types of segmentation:

Class Segmentation: A segmentation result that identifies, locates and classifies every pixel every object within an image according to the class they belong to.

Object Segmentation: A segmentation result that identifies, locates and classifies every pixel of every distinct object within an image.

###Task Description

The goal is to apply multi-class image segmentation to the Pascal VOC2009 dataset. The implementation specifics we have chosen are to apply Class rather than Object Segmentation.

###Segmentation General Principles

![alt text](https://miro.medium.com/max/1316/1*h2Zc1dsaCGW2IVWp7hvKAA.png)

Segmentation architectures are neural networks containing (typically) convolutional layers, pooling layers, batch normalization and non-linear activations in order to firstly classify and then locate those classifications. 

During training, it's usual to find that the initial layers learn broad features about the training images, wherea the later layers learn deeper features. For example, an initial layer might identify certain shapes, edges or colours, while later layers will identify specific objects or combinations of shapes. 

The nature of the downsampling process (done with the pooling layers, which act as a kind of filter) means that the low-level neurons contain information about a small region wheras the later neurons are concerned with a much wider region of the image. In this way, the image is continually decreased in size but the number of channels expands.

At this point, classification could be done. Classification is the process of adding a fully connected layer at the output of this section of network to remove locational information and result in a simple vector identifying recognised objects.

With segmentation, no fully connected layers are used in order to retain that spatial information. Instead, the low-resolution encoder output is now upsampled in a decoder section of the network along with convolutional layers. This upsampling can be done in multiple ways, and is an important decision in designed a segmentation network.

A common feature in segmentation networks is skip connections, as during downsampling and upsampling low-level information is not always retained. To avoid this, skip connections concatanate low-level features from the encoder network into the decoder network, allowing for more accurate segmentation boundaries.

###Dataset Overview

The Pascal VOC 2009 is a dataset of 14,743 images, of which a sub-section of 1,499 images is intended for training and testing segmentation networks.

For each of these images, a 'ground truth mask' has been hand-prepared, indicating for each pixel whether it is part of an instance of one of the 20 classes indicated in the dataset or not (in which case it is classified as 'background').

These training masks are then used for comparison during training with the masks prepared by the dataset, via techniques that will be described in greater depth later in this section.

The segmentation subset of the dataset comes pre-split into training and validation sets. The size of each sub-set is as follows:


*   Training set: 749 images
*   Validation set: 750 images

This is not a typical Training-Validation split, as typical percentage splits vary (according to dataset size and complexity) between 90/10 to 70/30 training to validation sub-set size. Despite this, for consistency and simplicity, it was chosen to maintain the pre-selected lists, as the purpose of this project was to demonstrate understanding of the underlying principles rather than to aim for highest quality segmentation prediction possible.

##Dataset processing

First, importing relevant libraries (in case not yet imported)
"""

import os
from skimage import io
import sys
import numpy as np
from numpy import argmax
import pickle
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.losses import *
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from sklearn.utils import class_weight
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import backend as K

"""The following functions and lists have the function of translating the ground truth masks contained in the dataset into usable masks for training purposes.

The included masks match each class-type with a specific RGB values for each classification, to make for easy visual seperation of the classes. For example, a train is indicated by an RGB value of R = 128, G = 192, B = 0.

The segmentation network requires these to be translated into class indices, as if they are left in the RGB format as the distortion of the different output sizes for each of the classes would lead to an unbalanced network and predictions.

Those class indices need to be further processes into one-hot encoding, or binary class indications, though this will be done later with the to_categorical function during final data input into the model.

To give an example of what these stages look like at a data level:



1.   Each mask is a matrix of size equal to its image. For each pixel there are three channels, indicating a red, green and blue value. Mask shape = 
(IMAGE_HEIGHT, IMAGE_WIDTH, 3)

2.   Each mask has been converted into class indices, an integer representation. A pixel holding a class would simply hold an integer value indicating that class. For example, if a particular pixel was showing a cow, and cows were indicated with the number 13, then that data-point would contain the value 13. Mask shape = (IMAGE_HEIGHT, IMAGE_WIDTH)

3. Each class indicator has been translated into a binary format, with 1 bit for each possible class that can be set to 1 if the pixel contains that particular class. In the case of the VOC2009 dataset, with 20 classes, this will result in 21 bits that can be set with the extra bit representing 'background' (or unclassified, in other words). 

  Our previous example of the cow would therefore see the value 13 translated into: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]. Mask shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 21).
"""

VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],
                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],
                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],
                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],
                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],
                [0, 64, 128]]

VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',
               'bottle', 'bus', 'car', 'cat', 'chair', 'cow',
               'diningtable', 'dog', 'horse', 'motorbike', 'person',
               'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']
    
def build_colormap2label():
    """Build an RGB color to label mapping for segmentation."""
    colormap2label = np.zeros(256 ** 3)
    for i, colormap in enumerate(VOC_COLORMAP):
        colormap2label[(colormap[0]*256 + colormap[1])*256 + colormap[2]] = i
    return colormap2label

def voc_label_indices(colormap, colormap2label):
  """Map an RGB color to a label."""
  colormap = colormap.astype(np.int32)
  idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256
        + colormap[:, :, 2])
  return colormap2label[idx]

"""With the following functions we extract the images from the VOC 2009 file structure. Using the skimage imread() function we read the images into data with the colour order Red-Green-Blue so no translation is needed for the mask colormap to classes as defined by the dataset (unlike cv2_imread(), which reads in BGR).

Additionally, at this point the images are resized to be a constant size, which is necessary for input into the segmentation network. While the images in the dataset have a constant width, they have a variable height.

As a compromise between memory usage and accuracy, the resized dimensions of 128x128 have been selected. As we encountered issues with the RAM allotment of Google Colab GPUs being exceeded causing a crash, we chose to keep a smaller value than would be optimal for maximum accuracy.

Importantly, the interpolation method is chosen to be INTER_NEAREST. While the effect on the images is minimal, it is crucial for the resizing of the masks. Any interpolation method that might cause some averaging of values would throw off the colour representation of the classes, and thus they would be misclassified. This would cause severe inaccuracies in the resulting ground-truth masks.
"""

IMG_DIM = 128   #Images resized to IMG_DIM*IMG_DIM
IMG_CHANNELS = 3   #RGB
NUM_TEST_IMAGES = 100

def extract_images(folder_path, file_names, test):
  images = []
  for path in file_names:
    image_path = (folder_path+"/"+path).rstrip()
    if test is False:
      image_path +=".jpg"
    images.append(cv2.resize(io.imread(image_path)[:,:,0:3]/255, (IMG_DIM, IMG_DIM), interpolation=cv2.INTER_NEAREST))
  return images

def extract_masks(folder_path, file_names):
  masks = []
  for path in file_names:
    masks_path = (folder_path+"/"+path).rstrip()+".png"
    masks.append(cv2.resize(io.imread(masks_path)[:,:,0:3], (IMG_DIM, IMG_DIM), interpolation=cv2.INTER_NEAREST))
  return masks

"""In the following section of code we build the lists of images:

*   Training images
*   Training masks
*   Validation images
*   Validation masks

Additionally, we form a list of test images for final testing of the network predictions. These images do not have corresponding masks and are extracted from the non-segmentation images in the VOC 2009 dataset. As they do not have corresponding masks the accuracy of the segmentation result cannot be easily judged mathematically, but does have intuitive value.

Once the training and validation mask images have been extracted, they are also translated into integer classification masks from their original RGB values.

"""

seg_image_path = voc_root_folder+"/JPEGImages"
mask_path = voc_root_folder+"/SegmentationClass"

#Form training image and mask lists
seg_train_text = voc_root_folder+"/ImageSets/Segmentation/train.txt"
seg_train_file = open(seg_train_text, 'r') 
seg_train_paths = seg_train_file.readlines()
train_inputs = extract_images(seg_image_path, seg_train_paths, False)
train_inputs = tf.convert_to_tensor(train_inputs)
train_masks_ims = extract_masks(mask_path, seg_train_paths)
train_masks = tf.convert_to_tensor([voc_label_indices(x, build_colormap2label()) for x in train_masks_ims])

#Form validation image and mask lists
seg_val_text = voc_root_folder+"/ImageSets/Segmentation/val.txt"
seg_val_file = open(seg_val_text, 'r') 
seg_val_paths = seg_val_file.readlines()
val_inputs = tf.convert_to_tensor(extract_images(seg_image_path, seg_val_paths, False))
val_masks_ims = extract_masks(mask_path, seg_val_paths)
val_masks = tf.convert_to_tensor([voc_label_indices(x, build_colormap2label())  for x in val_masks_ims])

image_names = sorted(os.listdir(seg_image_path))
collected_paths = np.concatenate((seg_train_paths, seg_val_paths))
collected_paths_adjusted = []
for path in collected_paths:
  path = path.rstrip() + ".jpg"
  collected_paths_adjusted.append(path)
image_names = [x for x in image_names if x not in collected_paths_adjusted]
image_names = image_names[0:NUM_TEST_IMAGES]
test_inputs = tf.convert_to_tensor(extract_images(seg_image_path, image_names, True))

"""##Segmentation Networks

The goal in this segmentation network is to implement a full network from scratch, with randomly allocated neuron weights. These will then be trained, according to the repeated cost-function comparison between the predicted masks from the network being contrasted with the actual ground-truth mask contained in the dataset. 

In the process of designing these networks, there were specific decisions that had to be made for which the background and reasoning will be identical for the 'from scratch' and transfer learning sections, so will not be repeated.

These are:
*   Cost and loss functions
*   Meta-parameters

###Meta-Parameters

Here, we choose certain meta-parameters which were adjusted to find an intuitive compromise for best results from the network.

1.   Activation function: Common activation functions include Sigmoid, ReLU, Leaky ReLU and many others. In this case ReLU was chosen, as it's both a simple function and one that escapes the issue of, for example, sigmoid, where large regions of the function do not allow for training. This is due to sigmoid having large flat output results for both strongly positive and strongly negative values. This can slow training or even prevent convergence.

    However, ReLU does have a weakness in that its negative region is flat. One possible network improvement would be to switch to Leaky-ReLU, where the flat slope of ReLU is replaced with a small positive slope. This could prevent neurons ending up negative inside the network and ceasing the train, essentially creating a less complex network architecture than was designed.

    This was not done due to time, as Keras has a slightly different implementation for Leaky-ReLU.

2.   Number of Epochs: This value was chosen as a compromise between overfitting the training data (from too many training iterations over the training set, causing the network to predict the training images near perfectly but not be general enough to accurately predict other images) and underfitting the data (from not enough training iterations leading to poor prediction of both the training images and others).

3.   Train size: A small change, the training dataset was reduced by one image in order to allow for more convenient batch size values than the number 749 which is not divisible by many possibilities. More batch size divisions were desirable for timing and RAM reasons, as will be explained further in the description of batch size.

4.    Batch size: The number of images that will actually be put through the network at each mini-iteration of prediction, back-propagation and weight adjustment. Each epoch (full iteration of the training dataset) will be divided into a set of batches of the given size. Too low a batch size will make training the dataset inconveniently slow. Too high will increase memory requirements unreasonably as the network will parameters for each image in the batch needed to be stored in active memory while the iteration is ongoing.
"""

#PARAMETERS
ACTIVATION_FUNCTION = 'relu'
NUM_CLASSES = 21
TRAIN_SIZE = 748
VAL_SIZE = 170
TRAIN_DATA_RANGE = slice(0,TRAIN_SIZE)
VAL_DATA_RANGE = slice(0, VAL_SIZE)
TRAIN_BATCH_SIZE = 17
VAL_BATCH_SIZE = 17

"""###Cost and Loss functions

In this section we define the dice coefficient, which is used as a metric to more accurately judge the quality of the predictions of the network. This is used as more basic measures like accuracy are not particularly useful with segmentation, especially given that the majority of pixels in a training dataset will often be 'Background' (signifying not a known classification). A prediction that therefore predicts all pixels as background will give a deceptively high accuracy result, while still being functionally useless.

The dice coefficient is given by the formula:

$$ 2\frac{|X\cap Y|}{|X|+|Y|} $$

The set intersection is implemented as a multiplication and the set vector length is given by a sum due as the predictions are in the form of one-hot encoded vectors.

The goal of the dice coefficient is (similar to another common metric, the Intersection Over Union) to give the similarity between two sets - the ground truth and the prediction. Where the two are identical, the result will be 1.0, and where they are completely un-alike it will be 0.0.

As this is a multi-class segmentation network it is also necessary to find the mean over all classes for the final result.

Additionally, we declare the loss metric for network training to be categorical crossentropy. This is effectively a Softmax activation - which squashes a vector of probabilities into the range 0-1 - and a Cross-entropy loss. This trains the Segmentation network to output the probability over the number of possible classes for each image.
"""

def dice_coef(y_true, y_pred, smooth=1e-6):
  intersection = K.sum(y_true * y_pred, axis=[1,2,3])
  union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])
  dice = K.mean((2. * intersection + smooth)/(union + smooth), axis=0)
  return dice

loss_metric = tf.keras.losses.categorical_crossentropy

"""###Model 3: By Scratch Segmentation

The goal in this segmentation network is to implement a full network from scratch, with randomly allocated neuron weights. These will then be trained, according to the repeated cost-function comparison between the predicted masks from the network being contrasted with the actual ground-truth mask contained in the dataset.

####Network Architecture

Many segmentation models have been developed in recent years, designed for differing levels of accuracy, ease-of-training and image topic specialisation (medical, indoors/outdoors, etc.)

The network architecture implemented here is a version of the UNet architecture, which was selected primarily because it's a popular segmentation architecture with plentiful documentation available and because its symmetrical structure gives a very intuitive understanding of how segmentation works. As the purpose of this project was understanding rather than optimal accuracy, this fit purposes well.

A brief summary of other architectures that were considered:

**FCN**: An early model for segmentation, the essential idea is to use a (usually pre-trained) classification network and convert it to fully convolutional by replacing the fully connected layers. As a result, rather than one predcted label for an input image, there are many labels. Upsampling that output via deconvolution then enables us to create the pixel-wise output (or mask). 

In order to maintain some of the location information that is lost in this process, skip connections are used between the shallower layers where there is more spatial location information but less information about deep features and the output.

In this way, via element-wise addition, the features extracted from the deeper layers can be used for classification while the shallower layers can provide location information for a more precise output segmentation mask.

**SegNet**: A similar structure to U-Net, with a successive encoder and decoder network structure along with pixelwise classification at the output layer. The encoder uses the VGG-16 convolutional layers (discarding the fully connected layers as per FCN) along with a decoder involving convolutions and upsampling via pooling layers. 

Pooling is the process of creating feature maps that summarise where features are found in the input, via a kind of learning digital filter that is applied after a non-linearity (the activation function, typically). The locations of the 2x2 max pooling layers are stored in memory, and these are then used in the decoder for upsampling. Finally, an n-class softmax classifier is used to output the pixel-wise classification results.

#####U-Net

U-net uses a symmetrical encoder and decoder structure, similarly to Segnet. The downsampling decoder, which examines progressively deeper layers of features to allow for classification is similar to FCN, being fully convolutional.
![U-net structure](https://lh3.googleusercontent.com/VLjiJdyykXtLvVHdc8bvaxGOg0WqThtuUdJ6os_LbLVlh2vhXEp3Qjs9dZr1Mqd23hG2ZRhorUMHfXn6DDYcefR7B4OMG90_2Df-F0SelkWLZKno4I6CauAlBZz0EtnmKRGWkNSd_Q=w2400)*O. Ronneberger et al. (2015)*

The decoder is where the structure shifts from other FCN-based segmentation architectures, as the upsampling is done with entire feature maps - making the model larger and more memory-intensive but better at picking up fairly precise locational information and small details. This is in contrast with Segnet, where pooling indices are used instead, which act as a kind of 'locational summary' but are less detailed than the feature-maps of U-net. This is why U-net is particularly well-known for application in biomedical segmentation, where relatively small image details should not be filtered out as they may contain important diagnostic information.

Interestingly, it contains no fully-connected layers, reducing the number of parameters and therefore making it capable of training on relatively small datasets. This is one reason why it was selected as the network architecture we would implement. While the VOC 2009 dataset is not small, it seemed a reasonable precaution in our first segmentation implementation to select a tolerant architecture.

Additionally, U-net has gone on to be the basis of many more advanced segmentation architectures such as PSPNet and DeepLabv3, which have increased segmentation accuracies even further.
"""

def conv_block(input_tensor, num_filters):
  encoder = Conv2D(num_filters, (3, 3), padding='same')(input_tensor)
  encoder = BatchNormalization()(encoder)
  encoder = Activation(ACTIVATION_FUNCTION)(encoder)
  encoder = Conv2D(num_filters, (3, 3), padding='same')(encoder)
  encoder = BatchNormalization()(encoder)
  encoder = Activation(ACTIVATION_FUNCTION)(encoder)
  return encoder

def encoder_block(input_tensor, num_filters):
  encoder = conv_block(input_tensor, num_filters)
  encoder_pool = MaxPooling2D((2, 2), strides=(2, 2))(encoder)
  return encoder_pool, encoder

def decoder_block(input_tensor, concat_tensor, num_filters):
  decoder = Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)
  decoder = concatenate([concat_tensor, decoder], axis=-1)
  decoder = BatchNormalization()(decoder)
  encoder = Activation(ACTIVATION_FUNCTION)(decoder)
  decoder = Conv2D(num_filters, (3, 3), padding='same')(decoder)
  decoder = BatchNormalization()(decoder)
  encoder = Activation(ACTIVATION_FUNCTION)(decoder)
  decoder = Conv2D(num_filters, (3, 3), padding='same')(decoder)
  decoder = BatchNormalization()(decoder)
  encoder = Activation(ACTIVATION_FUNCTION)(decoder)
  return decoder

#Build model - U-net  
tf.keras.backend.clear_session()
inputs  = Input(shape = train_inputs.shape[1:4])
#Layer 1
encoder0_pool, encoder0 = encoder_block(inputs, 32)
#Layer 2
encoder1_pool, encoder1 = encoder_block(encoder0_pool, 64)
#Layer 3
encoder2_pool, encoder2 = encoder_block(encoder1_pool, 128)
#Layer 4
encoder3_pool, encoder3 = encoder_block(encoder2_pool, 256)
#Layer 5
encoder4_pool, encoder4 = encoder_block(encoder3_pool, 512)
#Layer 6
center = conv_block(encoder4_pool, 1024)
#Layer 7
decoder4 = decoder_block(center, encoder4, 512)
#Layer 8
decoder3 = decoder_block(decoder4, encoder3, 256)
#Layer 9
decoder2 = decoder_block(decoder3, encoder2, 128)
#Layer 10
decoder1 = decoder_block(decoder2, encoder1, 64)
#Layer 11
decoder0 = decoder_block(decoder1, encoder0, 32)
#Layer 12
outputs = Conv2D(NUM_CLASSES, (1, 1), activation='softmax')(decoder0)

model_scratch = Model(inputs, outputs)
model_scratch.summary()

"""#####Training (Careful running code-block!)

In this section the training of the network to produce new model weights is done. 

 * Firstly, image generators are set up to perform common operations across each batch of image, such as rescaling, shifting, flipping and rotating the images. This has the effect of 'increasing' the number of training images seen by the network, resisting overfitting and forcing it to find a more general solution for mask prediction.

  A seed is used to ensure consistent transformations between the images and their masks. If the mask underwent different operations, the relationship between the images and masks would be lost, leading to highly inaccurate training.

* The optimizer and learning rate are selected - Adam, a common optimizer choice is used.

* An earlystopper is created, to prevent excessive training iterations when the network is no longer training usefully.

After these steps the actual training of the network is done with fit_generator (for more efficient memory usage vs fit() ) and the final model weights are saved.


"""

image_gen_args = dict(
	rotation_range=30,
	zoom_range=0.15,
	width_shift_range=0.2,
	height_shift_range=0.2,
	shear_range=0.15,
	horizontal_flip=True,
	fill_mode="nearest")

NUM_EPOCHS_FROM_SCRATCH = 400
SEED = 100
image_gen = ImageDataGenerator(**image_gen_args)
mask_gen = ImageDataGenerator(**image_gen_args)
image_gen.fit(train_inputs[TRAIN_DATA_RANGE].numpy())
mask_gen.fit(to_categorical(train_masks[TRAIN_DATA_RANGE].numpy()))

train_image_it = image_gen.flow(train_inputs[TRAIN_DATA_RANGE].numpy(), seed = SEED, batch_size = TRAIN_BATCH_SIZE)
train_mask_it = mask_gen.flow(to_categorical(train_masks[TRAIN_DATA_RANGE].numpy()), seed = SEED, batch_size = TRAIN_BATCH_SIZE)
val_image_it = image_gen.flow(val_inputs[TRAIN_DATA_RANGE].numpy(), seed = SEED, batch_size = VAL_BATCH_SIZE)
val_mask_it = mask_gen.flow(to_categorical(val_masks[TRAIN_DATA_RANGE].numpy()), seed = SEED, batch_size = VAL_BATCH_SIZE)
train_it = zip(train_image_it, train_mask_it)
val_it = zip(val_image_it, val_mask_it)

optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)
model_scratch.compile(loss= loss_metric, 
                      metrics=[dice_coef, tf.keras.metrics.categorical_accuracy, loss_metric], 
                      optimizer = optimizer)

checkpoint = ModelCheckpoint(base_path+"/segmentation_model_3_fromscratch_check.h5", monitor='loss', verbose=1, save_best_only=True, mode='min')
earlystopper = EarlyStopping(patience=10, verbose=1, monitor = 'loss')

#results = model_scratch.fit_generator(train_it, validation_data = val_it,
#                                      epochs = NUM_EPOCHS_FROM_SCRATCH, 
#                                      steps_per_epoch = train_inputs[TRAIN_DATA_RANGE].shape[0]/TRAIN_BATCH_SIZE, 
#																			callbacks = [checkpoint], 
#                                      validation_steps = val_inputs[VAL_DATA_RANGE].shape[0]/VAL_BATCH_SIZE)

#results_json = model_scratch.to_json()

#with open(base_path+'/history_model_3_segmentation', 'wb') as file_pi:
#  pickle.dump(results.history, file_pi)

#model_scratch.save_weights(base_path+"/segmentation_model_3_fromscratch.h5")

"""####Results

For the results, we take the trained model and do a set of predictions of validation and testing images, then show the resulting masks alongside the image for visual inspection. Additionally, for the validation images that have ground truth masks, we show the dice coefficient value of the two to give a measure of how precisely the prediction has recreated the original mask.

Also, we show a graph of the training progression of the network, to look for signs of underfitting and overfitting and other general trends.

Overall, the results from the 'from scratch network' were disappointing, with poor locational placement and classification on the validation images. The training masks are considerably more accurate than the validation attempts, but there appear to be few signs of actual overfitting, which has likely been partially mitigated by the image augmentation that was done. As the validation metrics level-out but never significantly reduce in exchange for an increase in training set metrics, this further signifies that overfitting was not a significant factor.

There are some signs that the segmentation network is learning, tending to cluster its classified pixels in the region of objects that should be classified, it's rare for the classification to be certain. The manner in which it locates its classifications around small features in highly localised areas may imply that the encoder is only managing to extract highly localised, small-scale features and basing its classifications only on those, rather than deeper features in the later layers. The weakness of the classifier in this case may imply that the later transfer learning network will be significantly better, as the encoder will already be a well-trained classifier before training begins.

The process of testing this network was made more problematic by repeated crashed during training, due to what appeared to be a memory leak. As a result, multiple compromises had to be made for efficiency (such as small batch sizes, slowing training and therefore slowing experimentation). While this made it difficult to problem-solve the network performance, it may indicate a mistake that has been made in the data-input to the network, leading to this memory leak. As of the time of this writing, that problem has not been found however, despite repeated attempts, so any impact a possible error may be having on the network performance is not able to be commented on.
"""

def create_colormap(mask):
  image = np.zeros( (IMG_DIM,IMG_DIM,IMG_CHANNELS) )  # black RGB image
  for i in range(image.shape[0]) :
      for j in range(image.shape[1]) :
          color_index = mask[i,j]  # index of '1' value
          image[i,j] = VOC_COLORMAP[ color_index ]
  return image

def create_mask(pred_mask):
  return np.argmax(pred_mask, axis = (len(pred_mask.shape)-1))

def show_predictions(dataset=None, num=1):
  for image in dataset:
    pred_mask = model.predict(image)
    display([image[0], create_mask(pred_mask)])

def iou_coef(y_true, y_pred, smooth=1):
  intersection = K.sum(K.abs(y_true * y_pred), axis=[1,2,3])
  union = K.sum(y_true,[1,2,3])+K.sum(y_pred,[1,2,3])-intersection
  iou = K.mean((intersection + smooth) / (union + smooth), axis=0)
  return iou

epochs = range(NUM_EPOCHS_FROM_SCRATCH)
DISPLAY_LIMIT = 20
DISPLAY_RANGE = slice(0, DISPLAY_LIMIT)

pickle_in = open(base_path+'/history_model_3_segmentation.json',"rb")
history = pickle.load(pickle_in)

plt.figure()
plt.plot(epochs, history['loss'], 'r', label='Training loss')
plt.plot(epochs,  history['val_loss'], 'b', label='Validation loss')
plt.plot(epochs,  history['dice_coef'], 'ro', label='Training dice coefficient')
plt.plot(epochs,  history['val_dice_coef'], 'bo', label='Validation dice coefficient')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss Value')
plt.ylim([0, 1])
plt.legend()
plt.show()

model_scratch.load_weights(base_path+"/segmentation_model_3_fromscratch.h5") 

pred_mask = model_scratch.predict(train_inputs[DISPLAY_RANGE].numpy())
for num in range(0, DISPLAY_LIMIT):
  pred_mask_t = create_mask(pred_mask[num])
  pred_mask_color = create_colormap(pred_mask_t).astype(np.uint8)
  pred_mask_color = cv2.cvtColor(pred_mask_color, cv2.COLOR_RGB2BGR)
  cv2_imshow(np.concatenate((cv2.cvtColor((train_inputs[num].numpy()*255).astype(np.uint8), cv2.COLOR_RGB2BGR), pred_mask_color), axis = 1))
  print("The dice coefficient value of the above mask is", iou_coef(tf.convert_to_tensor(np.expand_dims(to_categorical(train_masks[num].numpy(), num_classes = 21), axis = 0)), tf.expand_dims(pred_mask[num], axis = 0)))


print("\\\\\\\\\\")
print("\\\\\\\\\\")
print("Validation input predictions")
pred_mask = model_scratch.predict(val_inputs[DISPLAY_RANGE].numpy())
for num in range(0, DISPLAY_LIMIT):
  pred_mask_t = create_mask(pred_mask[num])
  pred_mask_color = create_colormap(pred_mask_t).astype(np.uint8)
  pred_mask_color = cv2.cvtColor(pred_mask_color, cv2.COLOR_RGB2BGR)
  cv2_imshow(np.concatenate((cv2.cvtColor((val_inputs[num].numpy()*255).astype(np.uint8), cv2.COLOR_RGB2BGR), pred_mask_color), axis = 1))
  print("The dice coefficient value of the above mask is", dice_coef(tf.convert_to_tensor(np.expand_dims(to_categorical(val_masks[num].numpy(), num_classes = 21), axis = 0)), tf.expand_dims(pred_mask[num], axis = 0)))

"""####Possible improvements

* The poor performance of the network may imply a fundamentally flawed network architecture. While numerous tutorials were used in implementing the U-Net architecture, it's possible that mistakes have been made. With more time, reimplementing the network from first steps (as while other architectures could be used, the performance from U-Net should be considerably better implying some error) could have a significant impact for increasing performance.

* An adaptive learning rate could both speed training and improve the quality of the network convergence, slowing the learning rate progressively as the network approaches its cost function minimum.

* The memory leaking issues required significant compromises in terms of reducing image size (which affects locational accuracy) and slowing training through small batch sizes (decreasing speed and therefore increasing prototyping time). If this was an issue in implementation rather than a back-end problem, it should be found.

* Some research indicates that large kernel sizes may improve segmentation results rather than the standard, small kernel sizes (such as 2x2 pooling as is implemented here). Segmentation requires both Classification and Localization of objects, which are in some ways contradictory processes. Large kernel sizes allow for denser connections between feature maps and per-pixel classifiers, potentially producing more accurate results.

* While the efforts with the classification networks showed weighting to have little impact, it may have a positive impact on the segmentation results as there are particularly strong class imbalances (such as between background and the other classes). The risk of the network falling to identifying primarily background and then being very reluctant to move away from that state (as it will at first have a negative impact on the results) leading to poor training is significant, and may be an element of the network's poor performance in this case.

###Model 4: Transfer Learning Segmentation

Transfer learning is the process of using weights that were pre-trained on (often large, but relatively generic) imagesets to recognise shallow and deeper features that can then be used for other applications with greater accuracy for the same length of training.

In the case of segmentation networks, this often takes advantage of the encoder-decoder structure of the networks, using existing trained encoder classification networks without the final fully connected layer and then connecting a decoder network that must be trained.

#####Network Architecture

######Mobile NetV2 and modified U-net

For this network we use a simple variant of a U-net model for better comparison with the 'From Scratch' model, though with a encoder now derived from MobileNetV2, a light-weight but efficient classification network which was designed with the idea of being attached to Classification, Detection or Segmentation networks.

![alt text](https://miro.medium.com/max/770/0*bLh_FRCym1UWnbpN.png)

A simplified U-net style decoder is then attached to the end.
"""

NUM_EPOCHS_TRANSFER = 200

def upsample(filters, size):
  initializer = tf.random_normal_initializer(0., 0.01)
  layer = tf.keras.Sequential()
  layer.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2,
                                      padding='same',
                                      kernel_initializer=initializer,
                                      use_bias=False))
  layer.add(tf.keras.layers.BatchNormalization())
  layer.add(tf.keras.layers.Dropout(0.3))
  layer.add(tf.keras.layers.ReLU())
  return layer

def simple_unet(output_channels):
  inputs = tf.keras.layers.Input(shape=[128, 128, 3])
  x = inputs

  # Downsampling through the model
  skips = down_stack(x)
  x = skips[-1]
  skips = reversed(skips[:-1])

  # Upsampling and establishing the skip connections
  for up, skip in zip(up_stack, skips):
    x = up(x)
    concat = tf.keras.layers.Concatenate()
    x = concat([x, skip])

  # This is the last layer of the model
  last = tf.keras.layers.Conv2DTranspose(
      output_channels, 3, strides=2,
      padding='same')  #64x64 -> 128x128

  x = last(x)

  return tf.keras.Model(inputs=inputs, outputs=x)

pretrained_model = tf.keras.applications.MobileNetV2(input_shape=[IMG_DIM, IMG_DIM, IMG_CHANNELS], include_top=False)

layers_to_retrieve = [
    'block_1_expand_relu', 
    'block_3_expand_relu',  
    'block_6_expand_relu',   
    'block_13_expand_relu', 
    'block_16_project',    
]
layers = [pretrained_model.get_layer(name).output for name in layers_to_retrieve]

encoder = tf.keras.Model(inputs=pretrained_model.input, outputs = layers)
encoder.trainable = False

decoder = [
  upsample(512, 3),
  upsample(256, 3),
  upsample(128, 3),
  upsample(64, 3),
]

inputs = tf.keras.layers.Input(shape = [IMG_DIM, IMG_DIM, IMG_CHANNELS])
skips = encoder(inputs)
outputs = skips[-1]
skips = reversed(skips[:-1])
for up, skip in zip(decoder, skips):
  outputs = up(outputs)
  concat = tf.keras.layers.Concatenate()
  outputs = concat([outputs, skip])

output_layer = tf.keras.layers.Conv2DTranspose(
    NUM_CLASSES, 3, strides=2, activation='softmax',
    padding='same')  

output = output_layer(outputs)
model_pretrained = tf.keras.Model(inputs = inputs, outputs = output)

optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)
pretrained_model.summary()
model_pretrained.summary()

"""######Training (Careful running code-block!)"""

image_gen_args = dict(
	rotation_range=30,
	zoom_range=0.15,
	width_shift_range=0.2,
	height_shift_range=0.2,
	shear_range=0.15,
	horizontal_flip=True,
	fill_mode="nearest")

SEED = 100
image_gen = ImageDataGenerator(**image_gen_args)
mask_gen = ImageDataGenerator(**image_gen_args)
image_gen.fit(train_inputs[TRAIN_DATA_RANGE].numpy())
mask_gen.fit(to_categorical(train_masks[TRAIN_DATA_RANGE].numpy()))

train_image_it = image_gen.flow(train_inputs[TRAIN_DATA_RANGE].numpy(), seed = SEED, batch_size = TRAIN_BATCH_SIZE)
train_mask_it = mask_gen.flow(to_categorical(train_masks[TRAIN_DATA_RANGE].numpy()), seed = SEED, batch_size = TRAIN_BATCH_SIZE)
val_image_it = image_gen.flow(val_inputs[TRAIN_DATA_RANGE].numpy(), seed = SEED, batch_size = VAL_BATCH_SIZE)
val_mask_it = mask_gen.flow(to_categorical(val_masks[TRAIN_DATA_RANGE].numpy()), seed = SEED, batch_size = VAL_BATCH_SIZE)
train_it = zip(train_image_it, train_mask_it)
val_it = zip(val_image_it, val_mask_it)

optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)
model_pretrained.compile(loss= loss_metric, 
                      metrics=[dice_coef, tf.keras.metrics.categorical_accuracy, loss_metric], 
                      optimizer = optimizer)

checkpoint = ModelCheckpoint(base_path+"/segmentation_model_4_pretrained_check.h5", monitor='val_dice_coef', verbose=1, save_best_only=True, mode='max')
#results = model_pretrained.fit_generator(train_it, validation_data = val_it,
#                                      epochs = NUM_EPOCHS_TRANSFER, 
#                                      steps_per_epoch = train_inputs[TRAIN_DATA_RANGE].shape[0]/TRAIN_BATCH_SIZE, 
#                                      callbacks = [checkpoint], 
#                                      validation_steps = val_inputs[VAL_DATA_RANGE].shape[0]/VAL_BATCH_SIZE)

#with open(base_path+'/history_model_4_segmentation', 'wb') as file_pi:
#  pickle.dump(results.history, file_pi)

#model_pretrained.save_weights(base_path+"/segmentation_model_4_pretrained.h5")

"""####Results

Below, we demonstrate a set of predictions in much the same way as was done with the 'By Scratch' network, with both training and validation masks shown next to their images, along with their dice coefficient score. The training mask predictions are shown to give a comparison between the more exact fitting of the training images which were used to configure the weights of the segmentation network (though less so with Transfer learning, as the encoder is not trained). 

We can immediately see the impact of the pre-trained classifier, as the inital dice coefficient values were above 0.7 and only increased marginally through the training. This could imply that the decoder architecture was not complex enough for the training to be beneficial. On the other hand, it could imply that the quality of the encoder acts as a bottleneck for network performance, and the decoder plays a small role in comparison (at least at this level of experience in developing segmentation networks).

When looking over the results, we find that the classification is broadly reasonable but the shapes do not correspond particularly well to the images they are intended to mask. That supports the notion that the decoder is the weakest point in the case of this network. We can also see that it is rather better at segmenting accurately in cases where there are few examples of objects in the image, and when objects take up a larger section of the image. The latter case might be improved by using a larger image size.

The final validation dice coefficient of approximately 0.805 with a loss of approximately 0.7 does seem to accurately depict the difference in quality with the training dice coefficient of approximately 0.88 with loss of 0.25. It's possible that with further training even better results could be achieved, though it already shows the capabilities of transfer learning to good effect that this training period - far shorter than that with the 'from scratch' network - is already creating a network with some basic segmentation capabilities.
"""

def create_colormap(mask):
  image = np.zeros( (IMG_DIM,IMG_DIM,IMG_CHANNELS) )  # black RGB image
  for i in range(image.shape[0]) :
      for j in range(image.shape[1]) :
          color_index = mask[i,j]  # index of '1' value
          image[i,j] = VOC_COLORMAP[ color_index ]
  return image

def create_mask(pred_mask):
  return np.argmax(pred_mask, axis = (len(pred_mask.shape)-1))

def show_predictions(dataset=None, num=1):
  for image in dataset:
    pred_mask = model.predict(image)
    display([image[0], create_mask(pred_mask)])

epochs = range(NUM_EPOCHS_TRANSFER)
DISPLAY_LIMIT = 40
DISPLAY_RANGE = slice(0, DISPLAY_LIMIT)

pickle_in = open(base_path+'/history_model_4_segmentation.json',"rb")
history = pickle.load(pickle_in)

plt.figure()
plt.plot(epochs, history['loss'], 'r', label='Training loss')
plt.plot(epochs,  history['val_loss'], 'b', label='Validation loss')
plt.plot(epochs,  history['dice_coef'], 'ro', label='Training dice coefficient')
plt.plot(epochs,  history['val_dice_coef'], 'bo', label='Validation dice coefficient')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss Value')
plt.ylim([0, 1])
plt.legend()
plt.show()

model_pretrained.load_weights(base_path+"/segmentation_model_4_pretrained.h5") 

print("Training input predictions")
pred_mask = model_pretrained.predict(train_inputs[DISPLAY_RANGE].numpy())
for num in range(0, DISPLAY_LIMIT):
  pred_mask_t = create_mask(pred_mask[num])
  pred_mask_color = create_colormap(pred_mask_t).astype(np.uint8)
  pred_mask_color = cv2.cvtColor(pred_mask_color, cv2.COLOR_RGB2BGR)
  cv2_imshow(np.concatenate((cv2.cvtColor((train_inputs[num].numpy()*255).astype(np.uint8), cv2.COLOR_RGB2BGR), pred_mask_color), axis = 1))
  print("The dice coefficient value of the above mask is", dice_coef(tf.convert_to_tensor(np.expand_dims(to_categorical(train_masks[num].numpy(), num_classes = 21), axis = 0)), tf.expand_dims(pred_mask[num], axis = 0)))

print("\\\\\\\\\\")
print("\\\\\\\\\\")
print("Validation input predictions")
pred_mask = model_pretrained.predict(val_inputs[DISPLAY_RANGE].numpy())
for num in range(0, DISPLAY_LIMIT):
  pred_mask_t = create_mask(pred_mask[num])
  pred_mask_color = create_colormap(pred_mask_t).astype(np.uint8)
  pred_mask_color = cv2.cvtColor(pred_mask_color, cv2.COLOR_RGB2BGR)
  cv2_imshow(np.concatenate((cv2.cvtColor((val_inputs[num].numpy()*255).astype(np.uint8), cv2.COLOR_RGB2BGR), pred_mask_color), axis = 1))
  print("The dice coefficient value of the above mask is", dice_coef(tf.convert_to_tensor(np.expand_dims(to_categorical(val_masks[num].numpy(), num_classes = 21), axis = 0)), tf.expand_dims(pred_mask[num], axis = 0)))

"""####Possible improvements

*   A more complex decoder may allow for more nuanced locational information for the pixel classifications. Currently the locational representation on the masks is one of the weakest factors with the results so far seen. It's possible 

*   A lower learning rate and longer training may show better results, as the history implies that the validation results did not significantly improve past a certain point. While this may indicate that the network was not sufficiently complex for the task, it may be that it was failing to converge further to a more accurate result.

* One issue that played an important role in the relatively small network (and image compression down to 128x128 was that the pre-trained network kept crashing due to excessive ram usage during training. While the solution wasn't found in time, if that problem could be solved and larger images used, that would have a significant contribution to the accuracy of the masks that the network produces.

* As with the 'From Scratch' network, it's possible that using Leaky ReLU may improve the performance of the network by preventing neurons from getting caught at zero and failing to train further.

##Conclusions

This was a challenging type of computer vision neural network to implement, expanding our knowledge of how deep learning works significantly, and providing practical experience of the challenges of problem solving with deep neural networks where problems are not immediately evident and testing has long turn-around times. 

The results were rather unsatisfactory from this section of the project, despite extensive efforts to find and improve the segmentation network performance. Still, a great deal was learned, firstly about the theory of segmentation networks and how they extend on the principles of classification networks to enhance a computer's understanding of not only what it is 'seeing', but also where those seen objects are. Secondly, it acted as a useful introduction to the deeper research that is being done in the field of computer vision and image segmentation, which is still very much at the forefront of computer vision application.

Our results neatly demonstrate the benefits of transfer learning, as the performance of the transfer learning network was far better despite a simpler network. The pre-training on a much larger image dataset produced a far more capable classifier than our short training time, and given classification is only the first step in creating a functional segmentation network (and the decoder is dependent on the quality of the encoder) it shows the value of using exisiting networks in cases where the dataset and/or training time is limited.

As was stated in the 'Possible Improvements' sections for each network, there are a wide variety of possible avenues for improving the network performance. Putting aside the consideration, for the sake of hypothesis, that there may be errors in the network implementations which may the root of the poor performance, one area which may have significant effect on performance is class weighting. 

While not successful for the classification network, the sharp difference in performance between the from scratch and transfer learning networks, particularly with the classifier, indicates that the from scratch network encoder was having significant issues distinguishing between the classes consistently, and often showed a tendancy to classify all pixels as background. This is likely due to the class imbalance, and may be leading network neurons to become stuck at zero (thanks to the behaviour of the ReLU activation function), simplifying the network and causing it to have trouble distinguishing between the comparatively small number of non-background classified pixels.

As a result, adding a weighted loss function would be the first action we would take to try and improve network performance, given the remaining time to do so. Additionally, some investigation of regularisation parameters could be beneficial.

# **5. Adversarial Examples**

This text is structured as follows:

1. Introduction to the theory + methodology (generation of deceptive labels).
2. Defining the adversarial encoder-decoder, coupling it with the classification network defined in 3. (from scratch), and training it on the deceptive labels (for the classifier's training data).
3. Evaluating our results via different visualizations.
4. Comparing the adversary from 2. with an adversary trained on the classifier's testing data.
5. Alternative routes: a look back on all the things we tried before arriving at the current version, and the things we tried to improve the current state.

As we will be working with the from-scratch-model `model_1` of chapter 3, we need to re-introduce the $128\times 128\times 3$ images at this stage.
"""

height = 128
width = 128
depth = 3
inputShape = (height, width, depth)

trainX = []
valX = []

for i, image in enumerate(x_train):
    trainX.append(cv2.resize(image, dsize=(width, height), interpolation=cv2.INTER_AREA))
trainX = np.array(trainX, dtype="float") / 255.0
trainY = np.array(labels_train)

for i, image in enumerate(x_val):
    valX.append(cv2.resize(image, dsize=(width, height), interpolation=cv2.INTER_AREA))
valX = np.array(valX, dtype="float") / 255.0
valY = np.array(labels_val)

testX = []

for i, image in enumerate(x_test):
    testX.append(cv2.resize(image, dsize=(width, height), interpolation=cv2.INTER_AREA))
testX = np.array(testX, dtype="float") / 255.0
testY = np.array(labels_test)


mlb = MultiLabelBinarizer()
trainY = mlb.fit_transform(trainY)
valY = mlb.fit_transform(valY)
testY = mlb.fit_transform(testY)

"""## 5.1 Introduction and methodology

In chapter 3, we developed a CNN and trained its weights ${\theta_c}$ such that it maps an input image to an output label with high accuracy:

$$ 
  h_{\theta_c}: \mathcal{X} \to \mathcal{Y}: 
  x \mapsto h_{\theta_c}(x) = y. 
$$

Here we defined the *input space* $\mathcal{X}\subseteq\mathbb{R}^{d\times d\times 3}$, with $d=128$, and the *output space* $\mathcal{Y}\subseteq\{0,1\}^K$, with $K=20$.

---

In this section, we develop an adversarial encoder-decoder generating a noise perturbation:

$$ 
  f_{\theta_a}: \mathcal{X}\to\mathcal{X}: 
  x \mapsto f_{\theta_a}(x) = \delta. 
$$

This encoder-decoder $f_{\theta_a} = \psi\circ\phi$ consists of two parts: the *encoder* and the *decoder*:
$$ \phi: \mathcal{X} \to \mathcal{F} $$
$$ \psi: \mathcal{F} \to \mathcal{X} $$
$$ \phi, \psi = \arg\min_{\phi,\psi} \mathcal{L}$$
Here, $\mathcal{L}$ is the dedicated loss function and $\mathcal{F}\in\mathbb{R}^p$ the *latent space*. 

---

The weights $\theta_a$ should be trained via backpropagation to serve two goals simultaneously:

*   To the naked eye, $x$ and $x' := x + \delta$ should still be very similar images, with the same object(s) clearly visible in both.
*   We want to find $\delta$ that is able to "fool" the CNN $h_{\theta_c}$ into believing $x + \delta$ has a different label than $x$, i.e. $h_{\theta_c}(x) \ne h_{\theta_c}(x+f_{\theta_a}(x))$. 
More specifically, we want to convince the model that if $x$ is an image of a cat, then $x'$ is an image of a dog.

A natural way to solve the first constraint is by adding a regularization term to the loss function, enforcing the outputs of the encoder-decoder (the noise perturbations) to be small.
This introduces a new hyperparameter $\lambda$, indicating how important we find it that our perturbations are small.
As we will see later however, this first constraint also solves itself somewhat.

For the second point, we generate a series of *deceptive labels*, in which every occurence of a *cat* in some image has its label replaced by *dog*.
Note that our training data therefore no longer has any images labeled as *cat*.
These deceptive labels can then be used for training the full adversarial network.
"""

# NOTE: we also do this for the test set, because of section 5.4

trainY_dec = trainY.copy()
testY_dec = testY.copy()

catInd = 7  # 8th class
dogInd = 11 # 12th class
assert mlb.classes_[catInd] == 'cat'
assert mlb.classes_[dogInd] == 'dog'

print("Train: ", end='')
for i in range(len(trainY_dec)):
  if trainY_dec[i][catInd] == 1:
    trainY_dec[i][catInd] = 0
    trainY_dec[i][dogInd] = 1
    print(i, end=", ") # Print the image indices that contained a cat

print("\nTest: ", end='')
for i in range(len(testY_dec)):
  if testY_dec[i][catInd] == 1:
    testY_dec[i][catInd] = 0
    testY_dec[i][dogInd] = 1
    print(i, end=", ") # Print the image indices that contained a cat

"""## 5.2 Adversarial network

### 5.2.1 Encoder-Decoder

As mentioned in 5.1, our adversarial noise will be generated by a convolutional encoder-decoder model $f_{\theta_a}$. 
We first define the architecture of this model.
The architecture was evaluated by its ability to function as an *autoencoder*, reconstructing the dataset with MSE loss as metric.
Although its purpose is quite different than this, we think this already indicates that the encoder-decoder is capable of grasping the important parts in this dataset.

First, we define a function `convLayer` to generate a general-purpose convolutional layer in our application.
Most of the parameters remain at their defaults (i.e. initializing weights and biases).
We use ReLU activation functions in between convlayers, and regularize the activities by $l_1$ or $l_2$.
This helps keep the output of each layer small.
Experimentation shows that this activity regularizer is most important in the last layer.

However, we also experienced that even without regularization, the output is quite small.
This is because we only adapted the labels corresponding to cats, and still desire that the other images are labeled correctly.
In itself, the adversarial encoder-decoder will thus try to not perturb the image *too* much.
However, perturbations can sometimes still be visibly changing the image, so we put a small default regularization term of $\lambda = 10^{-8}$ in each layer here.

Intuitively, one would expect the $l_2$ regularizer to give a more uniformous result, as outliers are severely punished (due to the squaring).
However, we did not experience much difference between $l_1$ or $l_2$, even when the regularization term $\lambda$ was much bigger.
"""

from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Dropout
from keras.models import Model
from keras import backend as K
from keras.regularizers import l1, l2, l1_l2

def convLayer(numFilters, name=None, filterSize=(3,3), reg=1e-8):
  return Conv2D(
      numFilters, filterSize, name=name,
      padding='same', activation='relu',
      activity_regularizer=l1(l=reg),
  )

"""Now it's time to define the full architecture of the encoder-decoder.
We also summarize the model, to neatly show our layerwise decisions.

In section 5.5 we discuss which different architectures of the network were tried out and how we eventually ended up with the current one.
"""

# adv_ED = adversarial Encoder-Decoder
adv_ED = Sequential(name='Encoder-Decoder')

# ENCODER
adv_ED.add(convLayer(50, filterSize=(5,5), name='CONV1'))
adv_ED.add(MaxPooling2D((2,2), padding='same'))
adv_ED.add(Dropout(0.1))

adv_ED.add(convLayer(100, filterSize=(5,5), name='CONV2'))
adv_ED.add(MaxPooling2D((2,2), padding='same'))
adv_ED.add(Dropout(0.1))

# Encoded representation is now (32, 32, 100) i.e. 204800-dimensional

# DECODER
adv_ED.add(convLayer(100, filterSize=(5,5), name='DECONV1'))
adv_ED.add(UpSampling2D((2,2)))
adv_ED.add(Dropout(0.1))

adv_ED.add(convLayer(50, filterSize=(5,5), name='DECONV2'))
adv_ED.add(UpSampling2D((2,2)))
adv_ED.add(Dropout(0.1))

adv_ED.add(Conv2D(3, (3, 3), activation='sigmoid', padding='same'))

adv_ED.build(input_shape=(None,)+trainX[0].shape)
adv_ED.summary()

"""### 5.2.2 Training full adversary
We are now ready to define the full adversarial model architecture. 
This consists of an `Input` image layer, which is first fed into the encoder-decoder model `adv_ED` defined in section 5.2.1.
The output of this (sequential) model is then summed with the input image again, to generate the perturbed image.
Passing this perturbed image to our CNN `model_1` trained from scratch (see section 3) then gives us the classification of the image.
We also make sure to freeze the weights of `model_1` here, as we don't want to train the classification network anymore. 
A summary of the adversarial model shows us that the weights were indeed correctly frozen.
"""

from keras.layers import Input, Add

# Construct the perturbed image and feed it to the classifier
image     = Input(shape=trainX[0].shape, name='input')
noise     = adv_ED(image)
perturbed = Add(name='add')([noise,image])
out       = model_1(perturbed)

# Build the full adversarial model from these layers (and freeze the classifier)
adv_model = Model( inputs=image, outputs=out )
adv_model.layers[-1].trainable = False
adv_model.compile(optimizer='adam', loss="binary_crossentropy", 
                  metrics=["accuracy", f1_m, precision_m, recall_m])

# Make a copy for later (section 5.4)
adv_model_onTest = adv_model

adv_model.summary()

"""Now it's time to train the model. 
Note that during backpropagation, the batches will first go through the convolutional classifier from section 3.
Therefore, this attack is a white-box attack: we are using the gradient information provided by the weights of the classification network when we apply backpropagation in our adversarial encoder-decoder.

As such, our approach is not really applicable in a real-world scenario, as one usually doesn't have the weight information we have here.
Of course, if our adversary ultimately generalizes well to unseen images, it might also generalize well to new networks.
However, we choose here to focus on the dataset at hand (see next paragraph).

We interpreted the assignment as follows: create a couple of images that (in our case) look like cats, but that are wrongfully classified as dogs by the CNN. In that sense, there was no reason for the adversarial autoencoder to generalize well to unseen images whatsoever, and as such we did not use the validation set to monitor the evolution of the loss during training. Instead, the aim was for the adversarial autoencoder to be fit specifically on the training data at hand, without the fear of overfitting, and as such obtain some dedicated images that maximally meet the assignments' requirements.
"""

# Early stopping -- loss
from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=4)

# Saving weights with smallest loss
from keras.callbacks import ModelCheckpoint
checkpoint = ModelCheckpoint('/content/VOCdevkit/models/adversarial_model_checkpoint.h5', 
    monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, 
    mode='auto', period=1)

numEpochs = 30

# # TRAINING
# adv_history = adv_model.fit(
#     x = trainX, y = trainY_dec,
#     epochs = numEpochs,
#     batch_size = 32,
#     shuffle = True,
#     callbacks = [es, checkpoint]
# )

# # Restore best weights
# adv_model.load_weights('/content/VOCdevkit/models/adversarial_model_checkpoint.h5')

# adv_model.save_weights("/content/VOCdevkit/models/advmodel_vfinal_big.h5")
adv_model.load_weights("/content/VOCdevkit/models/advmodel.h5")

"""## 5.3 Evaluation of results

### 5.3.1 Technical results

As with the training of any neural network, we could make some fancy plots to show us the accuracy evolution over time. 
However, this is unlikely to bring any insight in this case, as the network starts with a fairly high accuracy (we only adjusted some labels). 
As earlier discussed, accuracy also isn't the best metric here, so we just skip making plots here altogether.

Now we can test our adversary on some images from the training set.
We first define some convenience functions to extract the adversarial noise and the perturbed image and to display the top 5 in classification (for original and perturbed image).
All these functions have an optional parameter `adversary`, so we can reuse them in section 5.4 when evaluating the adversarial trained on the testing data of $h_{\theta_a}$.
"""

def perturb(img, adversary=adv_model):
  # Extracts noise and generates the adversarial image
  noiseLayer = Model(
      inputs  = adversary.get_layer('Encoder-Decoder').layers[0].input,
      outputs = adversary.get_layer('Encoder-Decoder').layers[-1].output
  )
  noise = noiseLayer.predict(img.reshape((1,128,128,3)))
  noise = noise.reshape(128,128,3)
  adv_img = img + noise
  return adv_img, noise

def showImgNoisePerturbed(img, adversary=adv_model):
  # Displays original image, noise and adversarial image side-by-side
  adv_img, noise = perturb(img, adversary)
  noise /= np.max(noise) # For display purposes
  cv2_imshow(np.hstack( (img*255, noise*255, adv_img*255) ))

def classifyTopFive(img, adversary=adv_model):
  # Prints classification info (top 5) for both images
  # Note: caps the image label to 10 characters for formatting
  adv_img, _ = perturb(img, adversary)
  img     =     img.reshape((1,)+trainX[0].shape)
  adv_img = adv_img.reshape((1,)+trainX[0].shape)

  probNormal = model_1.predict(img)[0]
  idxNormal = np.argsort(probNormal)[::-1][:5]
  probAdvers = model_1.predict(adv_img)[0]
  idxAdvers = np.argsort(probAdvers)[::-1][:5]

  print("Original" + "\t\t\t" + "Perturbed")
  for i in range(5):
    j = idxNormal[i]
    labelNormal = "{:.2f}% {}".format(probNormal[j] * 100, mlb.classes_[j][:8])
    j = idxAdvers[i]
    labelAdvers = "{:.2f}% {}".format(probAdvers[j] * 100, mlb.classes_[j][:8])
    print(labelNormal + "\t\t\t" + labelAdvers)

def evaluateAdversary(img, adversary=adv_model):
  showImgNoisePerturbed(img, adversary)
  classifyTopFive(img, adversary)

  # # Print some additional statistics about the generated noise
  # _, noise = perturb(img)
  # print("\nAdditional noise statistics")
  # print("Average of the noise (in [0,1]) = ", noise.mean())
  # print("Average of the noise / Average of the image (%) = ", 
  #       100*noise.mean()/img.mean())

"""First we check which images contain a cat or dog, and are thus interesting to look at."""

print("Cats at ", end='')
for i in range(len(trainY)):
  if trainY[i][catInd] == 1:
    print(i, end=', ')
print()
print("Dogs at ", end='')
for i in range(len(trainY)):
  if trainY[i][dogInd] == 1:
    print(i, end=', ')

"""**Discussion of results**

The results below show how even small perturbations can fool the CNN classifier substantially. While the original and perturbed images are nearly identical to the human eye, in all three examples we see major changes in posterior outputs of the classification model. In fact, for the latter two examples the classifier performs even better on the perturbed images than on the originals!

We note that the perturbations are quite small, even though the regularization parameter was not large. Indeed, if we increased the regularization more, the noise rapidly became identically zero. 

At the same time, we do recognize shapes in the perturbations that are related to shapes in the original images, such as the contours of the cat in the lower example. This clearly demonstrates that our model is picking up interesting regions in the image, and not simply generating the noise in a random or uniform way over the image.

We could use these perturbed images as new training data for our classifier, which could lead to iterative improvement between the adversary and the original network.
This idea is what lead to the development of Generative Adversarial Networks (GANs).
"""

evaluateAdversary(trainX[15])
evaluateAdversary(trainX[69])
evaluateAdversary(trainX[148])

"""### 5.3.2 Clean visualization

The previous visualizations were more of the debugging kind (i.e. they include all information, have small images, lots of numbers...)
In this section, we devise a more proper way of visualizing our results.
The results now focus solely on the classification results for the categories of *cat* and *dog*.

We again first define a convenience function that will plot the image, its generated noise and perturbed image, along with cat/dog classification in a neat layout.
"""

import matplotlib.pyplot as plt

def visualizeAdversary(img, adversarial=adv_model):
  adv_img, noise = perturb(img, adversarial)
  noise /= np.max(noise) # For display purposes

  # Calculate the classification probabilities
  predictShape = (1,) + img.shape # Required shape to feed through model
  prob_orig  = model_1.predict(    img.reshape(predictShape))[:, [catInd, dogInd]][0]
  prob_adv   = model_1.predict(adv_img.reshape(predictShape))[:, [catInd, dogInd]][0]
  prob_noise = model_1.predict(  noise.reshape(predictShape))[:, [catInd, dogInd]][0]

  # Show image + noise = adversarial image
  fig, axes = plt.subplots(2, 5, figsize=(15,6))
  axes[0,0].imshow(cv2.cvtColor(img.astype('float32'), cv2.COLOR_BGR2RGB))
  axes[0,1].text(x = 0.5, y = 0.5, s = "+", fontsize = 50, horizontalalignment = "center", verticalalignment = "center")
  axes[0,2].imshow(cv2.cvtColor(noise.astype('float32'), cv2.COLOR_BGR2RGB))
  axes[0,3].text(x = 0.5, y = 0.5, s = "=", fontsize = 50, horizontalalignment = "center", verticalalignment = "center")
  axes[0,4].imshow(cv2.cvtColor(adv_img.astype('float32'), cv2.COLOR_BGR2RGB))

  # Plot the cat/dog probabilities
  barlist = []
  barlist.append(axes[1,0].bar([0,1], prob_orig,  color = "teal"))
  barlist.append(axes[1,2].bar([0,1], prob_noise, color = "teal"))
  barlist.append(axes[1,4].bar([0,1], prob_adv,   color = "teal"))
  for bar in barlist:
    bar[1].set_color("darkorange")

  # Axes formatting
  for j in range(5):
    axes[0, j].axis('off')
  for j in [1, 3]:
    axes[1, j].axis('off')
  for j in [0, 2, 4]:
    axes[1, j].hlines(0.5, xmin = -1, xmax = 2, colors = "grey", linestyles = "--")
    axes[1, j].set_xticks([0,1])
    axes[1, j].set_xticklabels(["cat", "dog"])
    axes[1, j].set_ylim([0,1])

  plt.tight_layout()

"""We can now call this function on some images.

**Discussion of results**

Below we show in the first column three images of cats from the training dataset, underneath them we show the predictions of the CNN on these images, with the two bars indicating the likelihood of the image to be a cat or a dog, respectively. The middle column shows the same for the noise perturbations and the rightmost column shows the final perturbed images.

The results indicate that the intended objective was not met for these images: while the first goal of keeping the perturbations small is met, the perturbed images are not able to ``fool'' the CNN into believing they are in fact images of dogs. While some differences in output are possible, e.g. in the first image where the posterior probability of observing a cat drops from more than 60% to 30% while the picture remains almost identical to the human eye, the CNN could not be fooled into believing it was an image of a dog.
"""

visualizeAdversary(trainX[15])
visualizeAdversary(trainX[41])
visualizeAdversary(trainX[53])
plt.show()

"""### 5.3.3 In-depth analysis

Because these results aren't 100% satisfying, we take a more in-depth look at the generated noise and the underlying encoder-decoder.
We first take a sample image.
"""

img = trainX[148]
cv2_imshow(255*img)

"""The representation of this image in latent space $\mathcal{F}$ (as output of the encoder) is of dimension $32 \times 32 \times 100$.
Therefore, we can visualize this as $100$ filters of size $32 \times 32$.
We arrange them in a $10 \times 10$ grid.

**Observation**

A lot of these filters seem inactive, essentially capturing nothing...
"""

img = trainX[148]
latentLayer = Model(
    inputs  = adv_model.get_layer('Encoder-Decoder').layers[0].input,
    outputs = adv_model.get_layer('Encoder-Decoder').get_layer('DECONV1').input
)
latent = latentLayer.predict(img.reshape((1,128,128,3)))

print("Latent vector shape =", latent.shape)
print("Latent vector mean and standard deviation =", latent.mean(), latent.std())
print("Latent vector seen as convolutional filters:")

latent = latent.reshape(latent.shape[1:]) # Remove the leading (1,)

if latent.shape == (16,16,8):
  # Small model: just print filters as a row vector
  # TODO eventueel weg dit stuk? Als we kleine model ook weglaten in 5.2.1
  enlargedSize = 128
  filters = np.zeros((enlargedSize,1))
  for i in range(latent.shape[-1]):
    filt_i = cv2.resize(latent[:,:,i], (enlargedSize,enlargedSize), interpolation=cv2.INTER_NEAREST)
    filters = np.hstack((filters, filt_i/filt_i.max()))
  cv2_imshow(255*filters)

elif latent.shape == (32,32,100):
  # Big model: lots of filters, print them in a grid
  enlargedSize = 60
  colFilt = np.zeros((1,10*enlargedSize+1+10))
  for i in range(10):                  # Big model
    rowFilt = np.zeros((enlargedSize,1))
    for j in range(10):
      filt_ij = cv2.resize(latent[:,:,i+j*10], (enlargedSize,enlargedSize), interpolation=cv2.INTER_NEAREST)
      rowFilt = np.hstack((rowFilt, filt_ij/filt_ij.max(), np.ones((enlargedSize,1))))
    colFilt = np.vstack((colFilt, np.ones((1,colFilt.shape[1])), rowFilt))
  cv2_imshow(255*colFilt)

"""We can also look at some statistics about the generated noise.
This was important throughout working on the project, as it often indicated that the noise was uniform over the image (it had a standard deviation of $\mathcal{O}(10^{-8})$ and its variance was thus of the order of the machine epsilon).

Now, this noise looks more like actual noise (when recoded such that its maximal value corresponds to $1$).
"""

img = trainX[148]
noiseLayer = Model(
    inputs  = adv_model.get_layer('Encoder-Decoder').layers[0].input,
    outputs = adv_model.get_layer('Encoder-Decoder').layers[-1].output
)
noise = noiseLayer.predict(img.reshape((1,128,128,3)))
noise = noise.reshape((128,128,3))

for i in range(3):
  print("CHANNEL = " + ["B","G","R"][i])
  noise_i = noise.reshape((128,128,3))
  noise_i = noise[:,:,i]
  print("  Noise shape =", noise_i.shape)
  print("  Noise mean and standard deviation =", noise_i.mean(), noise_i.std())

print("Noise as image | Rescaled to [0,255]")
cv2_imshow(np.hstack((
    255*noise, 
    255*np.ones((noise.shape[0],1,3)), # Separating white line
    255*noise/noise.max()
)))

"""Finally, we can also visualize the convolutional filters of the first layer. 

"""

# The following variables are 4D tensors.
# - Dim 1&2 = filter size
# - Dim 3   = number of input channels
# - Dim 4   = number of output channels (or the number of filters)
conv1 = adv_model.get_layer('Encoder-Decoder').get_layer('CONV1').get_weights()[0]
conv2 = adv_model.get_layer('Encoder-Decoder').get_layer('CONV2').get_weights()[0]

print(conv1.shape) # 16 filters 3x3, 3 channels || 50 filters 5x5, 3 channels
print(conv2.shape) # 8 filters 3x3, 16 channels || 100 filters 5x5, 50 channels

# Visualizing the first convolutional layer
numFilters = conv1.shape[-1]
for i in range(numFilters):
  filt = conv1[:,:,:,i]
  filt = cv2.resize(filt, (60,60), interpolation=cv2.INTER_NEAREST)
  cv2_imshow(255*filt/filt.max())
  print("")

"""## 5.4 Applying the adversary to test data

In this section, we examine what the effect is when the adversary is trained on the test data, i.e. data that was not used when training the final layers of the adversarial network (the original CNN classifier).

Our initial expectation would be that it will be easier to fool on the test data, as the original classifier hasn't seen this data yet and is thus more prone to error. Or, in the ideal scenario where the CNN generalizes well to the test data, the results would be comparable with those in the previous section. However, the reality appeared to be somewhat different ...
"""

# Repeating the exact same training loop

# Early stopping -- loss
from keras.callbacks import EarlyStopping
es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=4)

# Saving weights with smallest loss
from keras.callbacks import ModelCheckpoint
checkpoint = ModelCheckpoint('/content/VOCdevkit/models/adversarial_model_checkpoint.h5', 
    monitor='loss', verbose=1, save_best_only=True, save_weights_only=True, 
    mode='auto', period=1)

numEpochs = 30

# # TRAINING
# adv_history = adv_model_onTest.fit(
#     x = testX, y = testY_dec,
#     epochs = numEpochs,
#     batch_size = 32,
#     shuffle = True,
#     callbacks = [checkpoint,es]
# )

# # Restore best weights
# adv_model_onTest.load_weights('/content/VOCdevkit/models/adversarial_model_checkpoint.h5')

# adv_model_onTest.save_weights("/content/VOCdevkit/models/advmodel_onTest.h5")
adv_model_onTest.load_weights("/content/VOCdevkit/models/advmodel_onTest.h5")

"""Checking out where all the cats at in the testing dataset."""

print("Cats at ", end='')
for i in range(len(testY)):
  if testY[i][catInd] == 1:
    print(i, end=', ')
print()
print("Dogs at ", end='')
for i in range(len(testY)):
  if testY[i][dogInd] == 1:
    print(i, end=', ')

"""**Discussion of the results**

Compared to the training data in section 5.3, we can see the perturbations are now suddenly much larger. A possible explanation may be that a larger regularization may be required, as the test set is much smaller than the training set, so there is much less data to train on and errors are made more easily. It's also possible that another network architecture is required for this smaller dataset.

However, changing the regularization or the architecture of the autoencoder `adv_model_onTest` prohibited us into comparing it directly with `adv_model`, which is why we kept it as it was without change.
"""

evaluateAdversary(testX[124], adv_model_onTest)

"""Adding the larger noise to the image changes the overall saturation of the image, but not much on the shapes that are in there: we still recognize the same cat in the perturbed image. However, while the CNN was 90% confident to see a cat in the original image, this likelihood dropped to 0% for the perturbed image! At the same time, the likelihood for a dog remained more or less stable (at a relatively low 10%)."""

visualizeAdversary(testX[16], adv_model_onTest)

"""## 5.5 Alternative routes

##### **Evolution of the autoencoder network structure**

The architecture used for the autoencoder model in 5.2.1 is based on the following conference paper, where a convolutional autoencoder network structure is developed for $256\times 256\times 3$ images of paintings:
> David, Omid E., and Nathan S. Netanyahu. "Deeppainter: Painter classification using deep convolutional autoencoders." *International conference on artificial neural networks. Springer*, Cham, 2016.

While our images were different in size, i.e. $128\times 128\times 3$, we felt the necessity to look up new architectures in literature, as the one we orginally applied did not yield good results. While it was based on a [keras blogpost by François Chollet](https://blog.keras.io/building-autoencoders-in-keras.html) giving a good introduction to (convolutional) autoencoders, it was designed specifically to work well for black-and-white MNIST images. Noteably, the filter size was smaller and the depth of the convolutional layers was also much smaller. We show this originally used architecture below:

```python
# adv_ED = adversarial Encoder-Decoder
adv_ED = Sequential(name='Encoder-Decoder')

# Convlayers used to be 16-8-8--8-8-16
# Now they're 32-16-8--8-16-32

# =======
# ENCODER
# Conv 1
adv_ED.add(convLayer(32, 'CONV1'))
adv_ED.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
adv_ED.add(Dropout(0.25))
# Conv 2
adv_ED.add(convLayer(16, 'CONV2'))
adv_ED.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
adv_ED.add(Dropout(0.25))
# Conv 3
adv_ED.add(convLayer(8, 'CONV3'))
adv_ED.add(MaxPooling2D(pool_size=(2, 2), padding='same'))
adv_ED.add(Dropout(0.25))

# Encoded representation is now (16,16,8), i.e. 2048-dimensional

# =======
# DECODER
# Deconv 1 (of course, this is not 'deconvolving' anything, I just mean
#           "decoder convolution")
adv_ED.add(convLayer(8, 'DECONV1'))
adv_ED.add(UpSampling2D(size=(2, 2)))
adv_ED.add(Dropout(0.25))
# Deconv 2
adv_ED.add(convLayer(16, 'DECONV2'))
adv_ED.add(UpSampling2D(size=(2, 2)))
adv_ED.add(Dropout(0.25))
# Deconv 3
adv_ED.add(convLayer(32, 'DECONV3'))
adv_ED.add(UpSampling2D(size=(2, 2)))
adv_ED.add(Dropout(0.25))

adv_ED.add(Conv2D(3, (3,3), activation='sigmoid', padding='same',
                  name='BACK2IM'))

adv_ED.build(input_shape=(None,)+trainX[0].shape)
adv_ED.summary()
```

Besides this, we also tried
1. a naive dense autoencoder with 1 hidden layer;
2. architectures which combine convolutional layers for the encoder and decoder parts with dense layers in between (in the latent space);
3. architectures developed for the ImageNet dataset found [here](https://siavashk.github.io/2016/02/22/autoencoder-imagenet/). 

All of them gave rise to many more parameters than the current model, with (1) resulting in terrible results and the results of (2) and (3) not better than our current results.

##### **Custom loss functions and evaluation metrics**

When brainstorming on how to tackle the problem, we also came accross a possible solution for the case in which the problem would be black-box and the weights of the CNN classifier would not be available.

In this case, we could again create an autoencoder to obtain a noise $\delta$ and perturbed image ${\bf x}' = {\bf x} + \delta$, but obviously the CNN network would not be available to attach to the autencoder and run through during backpropagation. So instead, one could use a custom loss function to train the autoencoder, such as

$$ 
\mathcal{L}({\bf{x}}, {\bf{x}}') 
= 
\textrm{I}[y_{\text{true}}^{(dog)}({\bf{x}}) = 1]
\times 
  \bigg\{\textrm{I}[y_{\text{pred}}^{(dog)}({\bf{x}}') > 0.5] 
  + \textrm{I}[y_{\text{pred}}^{(cat)}({\bf{x}}') < 0.5] \bigg\} 
+ \lambda ||{\bf{x}} - {\bf{x}}'||^2. 
$$

We tried to implement this but quickly ran into problems regarding implementation and doubts regarding the differentiability of the custom loss. We therefore abonded this route, as it also didn't align with the objective of a white-box attack.

As an alternative to the above, we did play with a custom loss function to optimize the existing model that we used:

```python
import keras.backend as K

def custom_loss(y_dec, y_pred):
  return (-1)*y_dec[:,dogInd]*K.log(y_pred[:,dogInd])
```

Clearly, this loss aims to maximally fool the CNN into believing cats are dogs. However, results did not show any improvement over simply using binary cross-entropy, so we didn't use it for the final implementation.

##### **Problems with uniform noise**

Stumbling our way through a decent architecture, we had the problem of noise that was perfectly uniform over the image (every pixel exactly the same color).
Turning regularization off seemed to resolve the problem, but then our results weren't very good. Furthermore, we also discovered that the produced noise often was exactly the same for each input image.

To resolve this problem, we tried the following things:
- Turning the use of a bias term on and off;
- *Kernel regularizers* are a general-purpose regularizer to keep the weights small. However, small weights don't mean that the produced output will be small! Certain combinations may still give rise to a large output. This is when we realized it was much better to use *activity regularizers*.
- While working with activity regularizers, small regularization did seem to have the desired effect of less totally-uniform noises, even on the original architecture. So after all, the architecture may not have been the problem!

Apart from the points above, we also tried the following things:
- Train the adversary not on the entire training dataset, but only on the subset consisting of images of cats. In this case perturbations were very large and mostly uniform, probably due to the same reason as in 5.4 (more regularization or different architecture necessary when dealing with smaller datasets, i.e. around 400 images vs more than 5000).
- Using an autoencoder that doesn't output noise, but directly the encoded image, as it would be for a classical autoencoder. This was actually one of the first ideas, as it also allowed to train it first as a classical autoencoder (minimizing the reconstruction error between original and decoded image), and use these weights as initial starting point for the adversarial autoencoder, having the same architecture. However, as this did not yield better results (no possibility to use activity regularization) and the assignment stated that the autoencoder should output noise, we omitted this from the final implementation.

##### **On the autoencoder**

As discussed, before moving on to the adversarial autoencoder, the encoder-decoder architecture was first applied to test its performance as a classical (convolutional) autoencoder. This already gave us useful information going forward in the assignment. Specifically, we found that while the decoded images were almost indistinguishable from the originals to the human eye, there were already some differences in CNN outputs (although relatively minor, and indeed unintended at this stage).

# **6. Discussion**

Lastly, we will briefly summarize what we have done and what we have learned while working on this project. The main goal of the exercise was to gain a fundamental insight of how deep learning is used in computer vision applications.  

**Data exploration**

First of all, we spent some time to inspect the used dataset and the type of data we were dealing with. The dataset was split into training, validation and test data as common practice. We noticed that an image could have 1 or more labels. This made already clear that we had to deal with multi-label classification, which influenced some design choices of the used deep learning neural networks. Besides, we noticed that there were strong class imbalances. On the one hand, some classes were much more present than other classes. On the other hand, there was the imbalance between positive and negative examples in many classes. This made clear that accuracy is not the best evaluation metric. Instead, we used precision, recall and most of all the F1-score, which is combining precision and recall in a single metric. 

**Classification**

Next, we constructed 2 convolutional neural networks for multi-label classification. The first one was created from scratch and was based on the principles of the famous VGG16 network. The second network was a pre-trained network which we finetuned. The pre-trained network was the VGG16 itself, trained on the huge ImageNet dataset. Accuracy and loss of the training and validation set were plotted to keep an eye on underfitting or overfitting. We could conclude that using a pre-trained network makes a large difference. The performance, evaluated by F1-score, was significantly higher when the network is already trained to capture some universal features in its first layers. It helped of course that the images in our dataset are similar in context than the ImageNet dataset. The higher convolutional layers on their turn were made trainable so that the network was there able to learn more dataset-specific high-level features.

During the creation of both classification networks, we explored quite some techniques and do’s and don’ts. For example, we considered to use weight classes, but eventually it didn’t really improve the performance. Another example is regularization, which is in fact very important. We have investigated some techniques, of which we implemented a thoughtful selection, e.g. the dropout and batch normalization layers help to improve generalization abilities of the networks. Also the use of data augmentation seemed to be a common practice to improve generalization.

**Segmentation**

We followed on from that to implement two segmentation networks, expanding on the idea of classification in computer vision to its next natural stages of then locating the objects within the image, and then classifying each pixel making up that object. 

Firstly, we made a network to be trained from scratch, for which we trialled multiple types of segmentation architecture. These included FCN, Segnet and finally settled on U-Net, a symmetrical encoder-decoder architecture commonly used in biomedical segmentation due its design specialising in not losing fine locational detailing. Here, we encountered some issues in getting the performance we would have liked, though in the process we evolved our architecture to include more regularization, attempted multiple forms of image augmentation and developed a good understanding of the essential underpinning theory of a segmentation network. Additionally, we developed some theories on how to improve our network - given adequate time - like the weighted loss function that proved unhelpful for the classification task might actually be relevant for segmentation due to the strong imbalance between the 'background' class and the other classes.

Next, we developed a segmentation network based on transfer learning, which takes advantage of the typical encoder-decoder structure of segmentation to use existing classification networks as encoders, already trained on extensive existing image datasets, but with the fully connected final layers removed and replaced with convolutional layers so as not to compress the output into a classification result. In our case, we used the Mobile Net v2 classification network, attached to a simplified U-net style decoder for better comparison with the results of the 'from scratch' network. While the pre-trained classifier should only be capable of classifying objects that were also in the datasets it was trained on, we could already clearly see the impact of transfer learning in the results, which were clearly better than the 'from scratch' results both in terms of metrics and visual accuracy.

Overall, we came away with a sense of the relative complexity of segmentation networks and the depth of the field for developing segmentation architectures. However, it was a great educational experience to better understand the implementation value of different layers and types of filtering in terms of the impact it has on segmenting the image, and how the features are extracted at the shallower vs deeper layers.

**Adversary**

Following the classification and segmentation tasks, we went on to the final part of the project: attempting to deceive the CNN classifier developed earlier by means of a so-called *adversarial attack*. To this end, we made use of *autoencoders*. These are network architectures that allow to take high-dimensional data and project this into a new latent space from which it can be accurately restored. Very often the high-dimensional data are images and the latent space is created through multiple convolutional layers, as was the case for this project. Typical applications are image compression, image denoising and feature extraction. In this project, we went one step further and developed a convolutional autoencoder with the aim of generating slightly different images compared to their originals. Ideally, the differences were minimal - barely visible to the human eye - but the original and perturbed images would have a signficantly different output when a prediction was performed on them by the CNN classifier. Throughout the notebook we referred to this novel application as an "adversarial autoencoder".

More specifically, our aim was to convince the classifier that images of cats actually contain dogs. Throughout model development, two constraints were kept in the back of our heads: (1) the CNN should be convinced that the perturbed image actually contains a dog and (2) the perturbed image should still look like a cat (actually, it should still look like the original image it was created from). The first constraint is the one we aimed to optimize towards, while the second one was fulfilled by means of regularization of the model outputs (ensuring the perturbation/noise was not too large, and the perturbed image did not deviate too much from the original).

The architecture of the encoder-decoder was decided by relying on literature and checking if this indeed gave decent results for our specific dataset by training it as a 'classical' autoencoder (minimizing reproduction error). We then implemented the actual "white-box" adversarial attack by attaching the layers of the CNN to the encoder-decoder structure. During training, these CNN weights would be frozen, thereby ensuring that we only trained on how the perturbed image should look like, and the actual classification rules remained unchanged. Of course we also introduced deceptive labels at this stage, which are identical to the true labels with the only exception that cats are replaced by dogs.

The results did disappoint us slightly. Despite trying a variety of different network architectures, playing with the regularization parameter along with the type of the regularization and experimenting with different loss functions we did not manage to effectively deceive the CNN in the way we wanted. Indeed, while the resulting images still clearly contained the same cats and the classifier often no longer recognized them as such, this didn't mean it suddenly mistook them for dogs. Maybe we built a classifier that worked too well and could not be fooled? Maybe there was still a better architecture or regularization out there that could do the job?

We did stumble upon some problems during the process of training the adversarial network, the most noteworthy being that the model really liked to return a uniform output, i.e. every pixel of the noise would have exactly the same RGB-value. Eventually, we managed to find the right set of parameters and were able to find perturbations that were meaningful, in the sense that they had some relation to the image from which they were derived.
"""